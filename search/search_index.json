{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AutoKeras: An AutoML system based on Keras. It is developed by DATA Lab at Texas A&M University. The goal of AutoKeras is to make machine learning accessible for everyone. Example Here is a short example of using the package. import autokeras as ak clf = ak . ImageClassifier () clf . fit ( x_train , y_train ) results = clf . predict ( x_test ) For detailed tutorial, please check here . Installation To install the package, please use the pip installation as follows: pip3 install autokeras Please follow the installation guide for more details. Note: Currently, AutoKeras is only compatible with Python >= 3.5 and TensorFlow >= 2.1.0 . Community Request an invitation . Use the #autokeras channel for communication. You can also follow us on Twitter @autokeras for the latest news. Contributors You can follow the Contributing Guide to become a contributor. Thank all the contributors! Backers We accept financial support on Open Collective . Thank every backer for supporting us! Cite this work Haifeng Jin, Qingquan Song, and Xia Hu. \"Auto-keras: An efficient neural architecture search system.\" Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2019. ( Download ) Biblatex entry: @inproceedings { jin2019auto , title = {Auto-Keras: An Efficient Neural Architecture Search System} , author = {Jin, Haifeng and Song, Qingquan and Hu, Xia} , booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining} , pages = {1946--1956} , year = {2019} , organization = {ACM} } DISCLAIMER Please note that this is a pre-release version of the AutoKeras which is still undergoing final testing before its official release. The website, its software and all content found on it are provided on an \"as is\" and \"as available\" basis. AutoKeras does not give any warranties, whether express or implied, as to the suitability or usability of the website, its software or any of its content. AutoKeras will not be liable for any loss, whether such loss is direct, indirect, special or consequential, suffered by any party as a result of their use of the libraries or content. Any usage of the libraries is done at the user's own risk and the user will be solely responsible for any damage to any computer system or loss of data that results from such activities. Should you encounter any bugs, glitches, lack of functionality or other problems on the website, please let us know immediately so we can rectify these accordingly. Your help in this regard is greatly appreciated. Acknowledgements The authors gratefully acknowledge the D3M program of the Defense Advanced Research Projects Agency (DARPA) administered through AFRL contract FA8750-17-2-0116; the Texas A&M College of Engineering, and Texas A&M.","title":"Home"},{"location":"#example","text":"Here is a short example of using the package. import autokeras as ak clf = ak . ImageClassifier () clf . fit ( x_train , y_train ) results = clf . predict ( x_test ) For detailed tutorial, please check here .","title":"Example"},{"location":"#installation","text":"To install the package, please use the pip installation as follows: pip3 install autokeras Please follow the installation guide for more details. Note: Currently, AutoKeras is only compatible with Python >= 3.5 and TensorFlow >= 2.1.0 .","title":"Installation"},{"location":"#community","text":"Request an invitation . Use the #autokeras channel for communication. You can also follow us on Twitter @autokeras for the latest news.","title":"Community"},{"location":"#contributors","text":"You can follow the Contributing Guide to become a contributor. Thank all the contributors!","title":"Contributors"},{"location":"#backers","text":"We accept financial support on Open Collective . Thank every backer for supporting us!","title":"Backers"},{"location":"#cite-this-work","text":"Haifeng Jin, Qingquan Song, and Xia Hu. \"Auto-keras: An efficient neural architecture search system.\" Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2019. ( Download ) Biblatex entry: @inproceedings { jin2019auto , title = {Auto-Keras: An Efficient Neural Architecture Search System} , author = {Jin, Haifeng and Song, Qingquan and Hu, Xia} , booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining} , pages = {1946--1956} , year = {2019} , organization = {ACM} }","title":"Cite this work"},{"location":"#disclaimer","text":"Please note that this is a pre-release version of the AutoKeras which is still undergoing final testing before its official release. The website, its software and all content found on it are provided on an \"as is\" and \"as available\" basis. AutoKeras does not give any warranties, whether express or implied, as to the suitability or usability of the website, its software or any of its content. AutoKeras will not be liable for any loss, whether such loss is direct, indirect, special or consequential, suffered by any party as a result of their use of the libraries or content. Any usage of the libraries is done at the user's own risk and the user will be solely responsible for any damage to any computer system or loss of data that results from such activities. Should you encounter any bugs, glitches, lack of functionality or other problems on the website, please let us know immediately so we can rectify these accordingly. Your help in this regard is greatly appreciated.","title":"DISCLAIMER"},{"location":"#acknowledgements","text":"The authors gratefully acknowledge the D3M program of the Defense Advanced Research Projects Agency (DARPA) administered through AFRL contract FA8750-17-2-0116; the Texas A&M College of Engineering, and Texas A&M.","title":"Acknowledgements"},{"location":"about/","text":"This package is developed by DATA LAB at Texas A&M University, collaborating with keras-team for version 1.0 and above. Core Team Haifeng Jin : Created, designed and implemented the AutoKeras system. Maintainer. Fran\u00e7ois Chollet : The API and system architecture design for AutoKeras 1.0. Code reviews for pull requests. Qingquan Song : Designed the neural architecture search algorithms. Implemented the tabular data classification and regression module. Xia \"Ben\" Hu : Project lead and maintainer.","title":"About"},{"location":"about/#core-team","text":"Haifeng Jin : Created, designed and implemented the AutoKeras system. Maintainer. Fran\u00e7ois Chollet : The API and system architecture design for AutoKeras 1.0. Code reviews for pull requests. Qingquan Song : Designed the neural architecture search algorithms. Implemented the tabular data classification and regression module. Xia \"Ben\" Hu : Project lead and maintainer.","title":"Core Team"},{"location":"auto_model/","text":"[source] AutoModel class autokeras . AutoModel ( inputs : Union [ autokeras . nodes . Input , List [ autokeras . nodes . Input ]], outputs : Union [ autokeras . engine . head . Head , autokeras . engine . node . Node , list ], name : str = \"auto_model\" , max_trials : int = 100 , directory : Union [ str , pathlib . Path , None ] = None , objective : str = \"val_loss\" , tuner : Union [ str , Type [ autokeras . engine . tuner . AutoTuner ]] = \"greedy\" , overwrite : bool = False , seed : Optional [ int ] = None , ) A Model defined by inputs and outputs. AutoModel combines a HyperModel and a Tuner to tune the HyperModel. The user can use it in a similar way to a Keras model since it also has fit() and predict() methods. The AutoModel has two use cases. In the first case, the user only specifies the input nodes and output heads of the AutoModel. The AutoModel infers the rest part of the model. In the second case, user can specify the high-level architecture of the AutoModel by connecting the Blocks with the functional API, which is the same as the Keras functional API . Example # The user only specifies the input nodes and output heads. import autokeras as ak ak . AutoModel ( inputs = [ ak . ImageInput (), ak . TextInput ()], outputs = [ ak . ClassificationHead (), ak . RegressionHead ()] ) # The user specifies the high-level architecture. import autokeras as ak image_input = ak . ImageInput () image_output = ak . ImageBlock ()( image_input ) text_input = ak . TextInput () text_output = ak . TextBlock ()( text_input ) output = ak . Merge ()([ image_output , text_output ]) classification_output = ak . ClassificationHead ()( output ) regression_output = ak . RegressionHead ()( output ) ak . AutoModel ( inputs = [ image_input , text_input ], outputs = [ classification_output , regression_output ] ) Arguments inputs : A list of Node instances. The input node(s) of the AutoModel. outputs : A list of Node or Head instances. The output node(s) or head(s) of the AutoModel. name : String. The name of the AutoModel. Defaults to 'auto_model'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. tuner : String or subclass of AutoTuner. If use string, it should be one of 'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a subclass of AutoTuner. Defaults to 'greedy'. overwrite : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source] fit method AutoModel . fit ( x = None , y = None , batch_size = 32 , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. y : numpy.ndarray or tensorflow.Dataset. Training data y. batch_size : Int. Number of samples per gradient update. Defaults to 32. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit. [source] predict method AutoModel . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Int. Number of samples per gradient update. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source] evaluate method AutoModel . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Int. Number of samples per gradient update. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source] export_model method AutoModel . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"AutoModel"},{"location":"auto_model/#automodel-class","text":"autokeras . AutoModel ( inputs : Union [ autokeras . nodes . Input , List [ autokeras . nodes . Input ]], outputs : Union [ autokeras . engine . head . Head , autokeras . engine . node . Node , list ], name : str = \"auto_model\" , max_trials : int = 100 , directory : Union [ str , pathlib . Path , None ] = None , objective : str = \"val_loss\" , tuner : Union [ str , Type [ autokeras . engine . tuner . AutoTuner ]] = \"greedy\" , overwrite : bool = False , seed : Optional [ int ] = None , ) A Model defined by inputs and outputs. AutoModel combines a HyperModel and a Tuner to tune the HyperModel. The user can use it in a similar way to a Keras model since it also has fit() and predict() methods. The AutoModel has two use cases. In the first case, the user only specifies the input nodes and output heads of the AutoModel. The AutoModel infers the rest part of the model. In the second case, user can specify the high-level architecture of the AutoModel by connecting the Blocks with the functional API, which is the same as the Keras functional API . Example # The user only specifies the input nodes and output heads. import autokeras as ak ak . AutoModel ( inputs = [ ak . ImageInput (), ak . TextInput ()], outputs = [ ak . ClassificationHead (), ak . RegressionHead ()] ) # The user specifies the high-level architecture. import autokeras as ak image_input = ak . ImageInput () image_output = ak . ImageBlock ()( image_input ) text_input = ak . TextInput () text_output = ak . TextBlock ()( text_input ) output = ak . Merge ()([ image_output , text_output ]) classification_output = ak . ClassificationHead ()( output ) regression_output = ak . RegressionHead ()( output ) ak . AutoModel ( inputs = [ image_input , text_input ], outputs = [ classification_output , regression_output ] ) Arguments inputs : A list of Node instances. The input node(s) of the AutoModel. outputs : A list of Node or Head instances. The output node(s) or head(s) of the AutoModel. name : String. The name of the AutoModel. Defaults to 'auto_model'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. tuner : String or subclass of AutoTuner. If use string, it should be one of 'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a subclass of AutoTuner. Defaults to 'greedy'. overwrite : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source]","title":"AutoModel class"},{"location":"auto_model/#fit-method","text":"AutoModel . fit ( x = None , y = None , batch_size = 32 , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. y : numpy.ndarray or tensorflow.Dataset. Training data y. batch_size : Int. Number of samples per gradient update. Defaults to 32. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit. [source]","title":"fit method"},{"location":"auto_model/#predict-method","text":"AutoModel . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Int. Number of samples per gradient update. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source]","title":"predict method"},{"location":"auto_model/#evaluate-method","text":"AutoModel . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Int. Number of samples per gradient update. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source]","title":"evaluate method"},{"location":"auto_model/#export_model-method","text":"AutoModel . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"export_model method"},{"location":"base/","text":"[source] Node class autokeras . Node ( shape = None ) The nodes in a network connecting the blocks. [source] Block class autokeras . Block ( name : str = None , ** kwargs ) The base class for different Block. The Block can be connected together to build the search space for an AutoModel. Notably, many args in the init function are defaults to be a tunable variable when not specified by the user. Arguments name : String. The name of the block. If unspecified, it will be set automatically with the class name. [source] build method Block . build ( hp , inputs = None ) Build the Block into a real Keras Model. The subclasses should override this function and return the output node. Arguments hp : HyperParameters. The hyperparameters for building the model. inputs : A list of input node(s). [source] Head class autokeras . Head ( loss = None , metrics = None , output_shape = None , ** kwargs ) Base class for the heads, e.g. classification, regression. Arguments loss : A Keras loss function. Defaults to None. If None, the loss will be inferred from the AutoModel. metrics : A list of Keras metrics. Defaults to None. If None, the metrics will be inferred from the AutoModel. output_shape : Tuple of int(s). Defaults to None. If None, the output shape will be inferred from the AutoModel.","title":"Base Classes"},{"location":"base/#node-class","text":"autokeras . Node ( shape = None ) The nodes in a network connecting the blocks. [source]","title":"Node class"},{"location":"base/#block-class","text":"autokeras . Block ( name : str = None , ** kwargs ) The base class for different Block. The Block can be connected together to build the search space for an AutoModel. Notably, many args in the init function are defaults to be a tunable variable when not specified by the user. Arguments name : String. The name of the block. If unspecified, it will be set automatically with the class name. [source]","title":"Block class"},{"location":"base/#build-method","text":"Block . build ( hp , inputs = None ) Build the Block into a real Keras Model. The subclasses should override this function and return the output node. Arguments hp : HyperParameters. The hyperparameters for building the model. inputs : A list of input node(s). [source]","title":"build method"},{"location":"base/#head-class","text":"autokeras . Head ( loss = None , metrics = None , output_shape = None , ** kwargs ) Base class for the heads, e.g. classification, regression. Arguments loss : A Keras loss function. Defaults to None. If None, the loss will be inferred from the AutoModel. metrics : A list of Keras metrics. Defaults to None. If None, the metrics will be inferred from the AutoModel. output_shape : Tuple of int(s). Defaults to None. If None, the output shape will be inferred from the AutoModel.","title":"Head class"},{"location":"block/","text":"[source] ConvBlock class autokeras . ConvBlock ( kernel_size = None , num_blocks = None , num_layers = None , max_pooling = None , separable = None , dropout_rate = None , ** kwargs ) Block for vanilla ConvNets. Arguments kernel_size : Int. If left unspecified, it will be tuned automatically. num_blocks : Int. The number of conv blocks, each of which may contain convolutional, max pooling, dropout, and activation. If left unspecified, it will be tuned automatically. num_layers : Int. The number of convolutional layers in each block. If left unspecified, it will be tuned automatically. max_pooling : Boolean. Whether to use max pooling layer in each block. If left unspecified, it will be tuned automatically. separable : Boolean. Whether to use separable conv layers. If left unspecified, it will be tuned automatically. dropout_rate : Float. Between 0 and 1. The dropout rate for after the convolutional layers. If left unspecified, it will be tuned automatically. [source] DenseBlock class autokeras . DenseBlock ( num_layers = None , use_batchnorm = None , dropout_rate = None , ** kwargs ) Block for Dense layers. Arguments num_layers : Int. The number of Dense layers in the block. If left unspecified, it will be tuned automatically. use_bn : Boolean. Whether to use BatchNormalization layers. If left unspecified, it will be tuned automatically. dropout_rate : Float. The dropout rate for the layers. If left unspecified, it will be tuned automatically. [source] Embedding class autokeras . Embedding ( max_features = 20000 , pretraining = None , embedding_dim = None , dropout_rate = None , ** kwargs ) Word embedding block for sequences. The input should be tokenized sequences with the same length, where each element of a sequence should be the index of the word. Arguments max_features : Int. Size of the vocabulary. Must be set if not using TextToIntSequence before this block. Defaults to 20000. pretraining : String. 'random' (use random weights instead any pretrained model), 'glove', 'fasttext' or 'word2vec'. Use pretrained word embedding. If left unspecified, it will be tuned automatically. embedding_dim : Int. If left unspecified, it will be tuned automatically. dropout_rate : Float. The dropout rate for after the Embedding layer. If left unspecified, it will be tuned automatically. [source] Merge class autokeras . Merge ( merge_type = None , ** kwargs ) Merge block to merge multiple nodes into one. Arguments merge_type : String. 'add' or 'concatenate'. If left unspecified, it will be tuned automatically. [source] ResNetBlock class autokeras . ResNetBlock ( version : str = None , pooling : str = None , ** kwargs ) Block for ResNet. Arguments version : String. 'v1', 'v2' or 'next'. The type of ResNet to use. If left unspecified, it will be tuned automatically. pooling : String. 'avg', 'max'. The type of pooling layer to use. If left unspecified, it will be tuned automatically. [source] RNNBlock class autokeras . RNNBlock ( return_sequences : bool = False , bidirectional : Optional [ bool ] = None , num_layers : Optional [ int ] = None , layer_type : Optional [ int ] = None , ** kwargs ) An RNN Block. Arguments return_sequences : Boolean. Whether to return the last output in the output sequence, or the full sequence. Defaults to False. bidirectional : Boolean. Bidirectional RNN. If left unspecified, it will be tuned automatically. num_layers : Int. The number of layers in RNN. If left unspecified, it will be tuned automatically. layer_type : String. 'gru' or 'lstm'. If left unspecified, it will be tuned automatically. [source] SpatialReduction class autokeras . SpatialReduction ( reduction_type : Optional [ str ] = None , ** kwargs ) Reduce the dimension of a spatial tensor, e.g. image, to a vector. Arguments reduction_type : String. 'flatten', 'global_max' or 'global_avg'. If left unspecified, it will be tuned automatically. [source] TemporalReduction class autokeras . TemporalReduction ( reduction_type = None , ** kwargs ) Reduce the dimension of a temporal tensor, e.g. output of RNN, to a vector. Arguments reduction_type : String. 'flatten', 'global_max' or 'global_avg'. If left unspecified, it will be tuned automatically. [source] XceptionBlock class autokeras . XceptionBlock ( activation = None , initial_strides = None , num_residual_blocks = None , pooling = None , ** kwargs ) XceptionBlock. An Xception structure, used for specifying your model with specific datasets. The original Xception architecture is from https://arxiv.org/abs/1610.02357. The data first goes through the entry flow, then through the middle flow which is repeated eight times, and finally through the exit flow. This XceptionBlock returns a similar architecture as Xception except without the last (optional) fully connected layer(s) and logistic regression. The size of this architecture could be decided by HyperParameters , to get an architecture with a half, an identical, or a double size of the original one. Arguments activation : String. 'selu' or 'relu'. If left unspecified, it will be tuned automatically. initial_strides : Int. If left unspecified, it will be tuned automatically. num_residual_blocks : Int. If left unspecified, it will be tuned automatically. pooling : String. 'ave', 'flatten', or 'max'. If left unspecified, it will be tuned automatically. [source] ImageBlock class autokeras . ImageBlock ( block_type = None , normalize = None , augment = None , ** kwargs ) Block for image data. The image blocks is a block choosing from ResNetBlock, XceptionBlock, ConvBlock, which is controlled by a hyperparameter, 'block_type'. Arguments block_type : String. 'resnet', 'xception', 'vanilla'. The type of Block to use. If unspecified, it will be tuned automatically. normalize : Boolean. Whether to channel-wise normalize the images. If unspecified, it will be tuned automatically. augment : Boolean. Whether to do image augmentation. If unspecified, it will be tuned automatically. [source] StructuredDataBlock class autokeras . StructuredDataBlock ( categorical_encoding = True , seed = None , ** kwargs ) Block for structured data. Arguments categorical_encoding : Boolean. Whether to use the CategoricalToNumerical to encode the categorical features to numerical features. Defaults to True. If specified as None, it will be tuned automatically. seed : Int. Random seed. [source] TextBlock class autokeras . TextBlock ( vectorizer = None , pretraining = None , ** kwargs ) Block for text data. Arguments vectorizer : String. 'sequence' or 'ngram'. If it is 'sequence', TextToIntSequence will be used. If it is 'ngram', TextToNgramVector will be used. If unspecified, it will be tuned automatically. pretraining : String. 'random' (use random weights instead any pretrained model), 'glove', 'fasttext' or 'word2vec'. Use pretrained word embedding. If left unspecified, it will be tuned automatically.","title":"Block"},{"location":"block/#convblock-class","text":"autokeras . ConvBlock ( kernel_size = None , num_blocks = None , num_layers = None , max_pooling = None , separable = None , dropout_rate = None , ** kwargs ) Block for vanilla ConvNets. Arguments kernel_size : Int. If left unspecified, it will be tuned automatically. num_blocks : Int. The number of conv blocks, each of which may contain convolutional, max pooling, dropout, and activation. If left unspecified, it will be tuned automatically. num_layers : Int. The number of convolutional layers in each block. If left unspecified, it will be tuned automatically. max_pooling : Boolean. Whether to use max pooling layer in each block. If left unspecified, it will be tuned automatically. separable : Boolean. Whether to use separable conv layers. If left unspecified, it will be tuned automatically. dropout_rate : Float. Between 0 and 1. The dropout rate for after the convolutional layers. If left unspecified, it will be tuned automatically. [source]","title":"ConvBlock class"},{"location":"block/#denseblock-class","text":"autokeras . DenseBlock ( num_layers = None , use_batchnorm = None , dropout_rate = None , ** kwargs ) Block for Dense layers. Arguments num_layers : Int. The number of Dense layers in the block. If left unspecified, it will be tuned automatically. use_bn : Boolean. Whether to use BatchNormalization layers. If left unspecified, it will be tuned automatically. dropout_rate : Float. The dropout rate for the layers. If left unspecified, it will be tuned automatically. [source]","title":"DenseBlock class"},{"location":"block/#embedding-class","text":"autokeras . Embedding ( max_features = 20000 , pretraining = None , embedding_dim = None , dropout_rate = None , ** kwargs ) Word embedding block for sequences. The input should be tokenized sequences with the same length, where each element of a sequence should be the index of the word. Arguments max_features : Int. Size of the vocabulary. Must be set if not using TextToIntSequence before this block. Defaults to 20000. pretraining : String. 'random' (use random weights instead any pretrained model), 'glove', 'fasttext' or 'word2vec'. Use pretrained word embedding. If left unspecified, it will be tuned automatically. embedding_dim : Int. If left unspecified, it will be tuned automatically. dropout_rate : Float. The dropout rate for after the Embedding layer. If left unspecified, it will be tuned automatically. [source]","title":"Embedding class"},{"location":"block/#merge-class","text":"autokeras . Merge ( merge_type = None , ** kwargs ) Merge block to merge multiple nodes into one. Arguments merge_type : String. 'add' or 'concatenate'. If left unspecified, it will be tuned automatically. [source]","title":"Merge class"},{"location":"block/#resnetblock-class","text":"autokeras . ResNetBlock ( version : str = None , pooling : str = None , ** kwargs ) Block for ResNet. Arguments version : String. 'v1', 'v2' or 'next'. The type of ResNet to use. If left unspecified, it will be tuned automatically. pooling : String. 'avg', 'max'. The type of pooling layer to use. If left unspecified, it will be tuned automatically. [source]","title":"ResNetBlock class"},{"location":"block/#rnnblock-class","text":"autokeras . RNNBlock ( return_sequences : bool = False , bidirectional : Optional [ bool ] = None , num_layers : Optional [ int ] = None , layer_type : Optional [ int ] = None , ** kwargs ) An RNN Block. Arguments return_sequences : Boolean. Whether to return the last output in the output sequence, or the full sequence. Defaults to False. bidirectional : Boolean. Bidirectional RNN. If left unspecified, it will be tuned automatically. num_layers : Int. The number of layers in RNN. If left unspecified, it will be tuned automatically. layer_type : String. 'gru' or 'lstm'. If left unspecified, it will be tuned automatically. [source]","title":"RNNBlock class"},{"location":"block/#spatialreduction-class","text":"autokeras . SpatialReduction ( reduction_type : Optional [ str ] = None , ** kwargs ) Reduce the dimension of a spatial tensor, e.g. image, to a vector. Arguments reduction_type : String. 'flatten', 'global_max' or 'global_avg'. If left unspecified, it will be tuned automatically. [source]","title":"SpatialReduction class"},{"location":"block/#temporalreduction-class","text":"autokeras . TemporalReduction ( reduction_type = None , ** kwargs ) Reduce the dimension of a temporal tensor, e.g. output of RNN, to a vector. Arguments reduction_type : String. 'flatten', 'global_max' or 'global_avg'. If left unspecified, it will be tuned automatically. [source]","title":"TemporalReduction class"},{"location":"block/#xceptionblock-class","text":"autokeras . XceptionBlock ( activation = None , initial_strides = None , num_residual_blocks = None , pooling = None , ** kwargs ) XceptionBlock. An Xception structure, used for specifying your model with specific datasets. The original Xception architecture is from https://arxiv.org/abs/1610.02357. The data first goes through the entry flow, then through the middle flow which is repeated eight times, and finally through the exit flow. This XceptionBlock returns a similar architecture as Xception except without the last (optional) fully connected layer(s) and logistic regression. The size of this architecture could be decided by HyperParameters , to get an architecture with a half, an identical, or a double size of the original one. Arguments activation : String. 'selu' or 'relu'. If left unspecified, it will be tuned automatically. initial_strides : Int. If left unspecified, it will be tuned automatically. num_residual_blocks : Int. If left unspecified, it will be tuned automatically. pooling : String. 'ave', 'flatten', or 'max'. If left unspecified, it will be tuned automatically. [source]","title":"XceptionBlock class"},{"location":"block/#imageblock-class","text":"autokeras . ImageBlock ( block_type = None , normalize = None , augment = None , ** kwargs ) Block for image data. The image blocks is a block choosing from ResNetBlock, XceptionBlock, ConvBlock, which is controlled by a hyperparameter, 'block_type'. Arguments block_type : String. 'resnet', 'xception', 'vanilla'. The type of Block to use. If unspecified, it will be tuned automatically. normalize : Boolean. Whether to channel-wise normalize the images. If unspecified, it will be tuned automatically. augment : Boolean. Whether to do image augmentation. If unspecified, it will be tuned automatically. [source]","title":"ImageBlock class"},{"location":"block/#structureddatablock-class","text":"autokeras . StructuredDataBlock ( categorical_encoding = True , seed = None , ** kwargs ) Block for structured data. Arguments categorical_encoding : Boolean. Whether to use the CategoricalToNumerical to encode the categorical features to numerical features. Defaults to True. If specified as None, it will be tuned automatically. seed : Int. Random seed. [source]","title":"StructuredDataBlock class"},{"location":"block/#textblock-class","text":"autokeras . TextBlock ( vectorizer = None , pretraining = None , ** kwargs ) Block for text data. Arguments vectorizer : String. 'sequence' or 'ngram'. If it is 'sequence', TextToIntSequence will be used. If it is 'ngram', TextToNgramVector will be used. If unspecified, it will be tuned automatically. pretraining : String. 'random' (use random weights instead any pretrained model), 'glove', 'fasttext' or 'word2vec'. Use pretrained word embedding. If left unspecified, it will be tuned automatically.","title":"TextBlock class"},{"location":"contributing/","text":"Contributing Guide Contributions are welcome, and greatly appreciated! Follow the tag of good first issue for the issues for beginner. Pull Request Guide Is this the first pull request that you're making with GitHub? If so, read the guide Making a pull request to an open-source project . Include \"resolves #issue_number\" in the description of the pull request if applicable. Briefly describe your contribution. Submit the pull request from the first day of your development and create it as a draft pull request . Click ready for review when finished and passed the all the checks. For the case of bug fixes, add new test cases which would fail before your bug fix. Setup Environment virtualenv Install Virtualenvwrapper . Create a new virtualenv named ak based on python3. mkvirtualenv - p python3 ak Please use this virtualenv for development. Installation Clone the repo. Go to the repo directory. Run the following commands. workon ak pip install - e \".[tests]\" pip uninstall autokeras add2virtualenv . pip install mkdocs pip install mkdocs - material pip install autopep8 pip install git + https : //github.com/gabrieldemarmiesse/typed_api.git@0.1.1 Run Tests Activate the virtualenv. Go to the repo directory Run the following lines to run the tests. Run all the tests. pytest tests Run all the unit tests. pytest tests / autokeras Run all the integration tests. pytest tests / integration_tests Code Style Option 1: Automatic It automatically formats the code every time you commit. To setup: 1. Install Docker. 2. Run cd .git/hooks && ln -s -f ../../shell/pre-commit.sh pre-commit from the repo directory. Option 2: Manually You can run the following manually every time you want to format your code. 1. Run shell/format.sh to format your code. 2. Run shell/lint.sh to check. Docstrings Docstrings should follow our style. Just check the style of other docstrings in AutoKeras.","title":"Contributing Guide"},{"location":"contributing/#contributing-guide","text":"Contributions are welcome, and greatly appreciated! Follow the tag of good first issue for the issues for beginner.","title":"Contributing Guide"},{"location":"contributing/#pull-request-guide","text":"Is this the first pull request that you're making with GitHub? If so, read the guide Making a pull request to an open-source project . Include \"resolves #issue_number\" in the description of the pull request if applicable. Briefly describe your contribution. Submit the pull request from the first day of your development and create it as a draft pull request . Click ready for review when finished and passed the all the checks. For the case of bug fixes, add new test cases which would fail before your bug fix.","title":"Pull Request Guide"},{"location":"contributing/#setup-environment","text":"","title":"Setup Environment"},{"location":"contributing/#virtualenv","text":"Install Virtualenvwrapper . Create a new virtualenv named ak based on python3. mkvirtualenv - p python3 ak Please use this virtualenv for development.","title":"virtualenv"},{"location":"contributing/#installation","text":"Clone the repo. Go to the repo directory. Run the following commands. workon ak pip install - e \".[tests]\" pip uninstall autokeras add2virtualenv . pip install mkdocs pip install mkdocs - material pip install autopep8 pip install git + https : //github.com/gabrieldemarmiesse/typed_api.git@0.1.1","title":"Installation"},{"location":"contributing/#run-tests","text":"Activate the virtualenv. Go to the repo directory Run the following lines to run the tests. Run all the tests. pytest tests Run all the unit tests. pytest tests / autokeras Run all the integration tests. pytest tests / integration_tests","title":"Run Tests"},{"location":"contributing/#code-style","text":"","title":"Code Style"},{"location":"contributing/#option-1-automatic","text":"It automatically formats the code every time you commit. To setup: 1. Install Docker. 2. Run cd .git/hooks && ln -s -f ../../shell/pre-commit.sh pre-commit from the repo directory.","title":"Option 1: Automatic"},{"location":"contributing/#option-2-manually","text":"You can run the following manually every time you want to format your code. 1. Run shell/format.sh to format your code. 2. Run shell/lint.sh to check.","title":"Option 2: Manually"},{"location":"contributing/#docstrings","text":"Docstrings should follow our style. Just check the style of other docstrings in AutoKeras.","title":"Docstrings"},{"location":"docker/","text":"Auto-Keras Docker Download Auto-Keras Docker image The following command download Auto-Keras docker image to your machine. docker pull garawalid / autokeras : latest Image releases are tagged using the following format: Tag Description latest Auto-Keras image devel Auto-Keras image that tracks Github repository Start Auto-Keras Docker container docker run - it --shm-size 2G garawalid/autokeras /bin/bash In case you need more memory to run the container, change the value of shm-size . ( Docker run reference ) Run application : To run a local script file.py using Auto-Keras within the container, mount the host directory -v hostDir:/app . docker run - it - v hostDir : / app --shm-size 2G garawalid/autokeras python file.py Example : Let's download the mnist example and run it within the container. Download the example : curl https : // raw . githubusercontent . com / keras - team / autokeras / master / examples / mnist . py --output mnist.py Run the mnist example : docker run - it - v \"$(pwd)\" : / app --shm-size 2G garawalid/autokeras python mnist.py","title":"Docker"},{"location":"docker/#auto-keras-docker","text":"","title":"Auto-Keras Docker"},{"location":"docker/#download-auto-keras-docker-image","text":"The following command download Auto-Keras docker image to your machine. docker pull garawalid / autokeras : latest Image releases are tagged using the following format: Tag Description latest Auto-Keras image devel Auto-Keras image that tracks Github repository","title":"Download Auto-Keras Docker image"},{"location":"docker/#start-auto-keras-docker-container","text":"docker run - it --shm-size 2G garawalid/autokeras /bin/bash In case you need more memory to run the container, change the value of shm-size . ( Docker run reference )","title":"Start Auto-Keras Docker container"},{"location":"docker/#run-application","text":"To run a local script file.py using Auto-Keras within the container, mount the host directory -v hostDir:/app . docker run - it - v hostDir : / app --shm-size 2G garawalid/autokeras python file.py","title":"Run application :"},{"location":"docker/#example","text":"Let's download the mnist example and run it within the container. Download the example : curl https : // raw . githubusercontent . com / keras - team / autokeras / master / examples / mnist . py --output mnist.py Run the mnist example : docker run - it - v \"$(pwd)\" : / app --shm-size 2G garawalid/autokeras python mnist.py","title":"Example :"},{"location":"head/","text":"[source] ClassificationHead class autokeras . ClassificationHead ( num_classes = None , multi_label = False , loss = None , metrics = None , dropout_rate = None , ** kwargs ) Classification Dense layers. Use sigmoid and binary crossentropy for binary classification and multi-label classification. Use softmax and categorical crossentropy for multi-class (more than 2) classification. Use Accuracy as metrics by default. The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification. The raw labels will be encoded to one column if two classes were found, or one-hot encoded if more than two classes were found. Arguments num_classes : Int. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use binary_crossentropy or categorical_crossentropy based on the number of classes. metrics : A list of Keras metrics. Defaults to use 'accuracy'. dropout_rate : Float. The dropout rate for the layers. If left unspecified, it will be tuned automatically. [source] RegressionHead class autokeras . RegressionHead ( output_dim = None , loss = \"mean_squared_error\" , metrics = None , dropout_rate = None , ** kwargs ) Regression Dense layers. The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be single-column or multi-column. The values should all be numerical. Arguments output_dim : Int. The number of output dimensions. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use mean_squared_error . metrics : A list of Keras metrics. Defaults to use mean_squared_error . dropout_rate : Float. The dropout rate for the layers. If left unspecified, it will be tuned automatically.","title":"Head"},{"location":"head/#classificationhead-class","text":"autokeras . ClassificationHead ( num_classes = None , multi_label = False , loss = None , metrics = None , dropout_rate = None , ** kwargs ) Classification Dense layers. Use sigmoid and binary crossentropy for binary classification and multi-label classification. Use softmax and categorical crossentropy for multi-class (more than 2) classification. Use Accuracy as metrics by default. The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification. The raw labels will be encoded to one column if two classes were found, or one-hot encoded if more than two classes were found. Arguments num_classes : Int. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use binary_crossentropy or categorical_crossentropy based on the number of classes. metrics : A list of Keras metrics. Defaults to use 'accuracy'. dropout_rate : Float. The dropout rate for the layers. If left unspecified, it will be tuned automatically. [source]","title":"ClassificationHead class"},{"location":"head/#regressionhead-class","text":"autokeras . RegressionHead ( output_dim = None , loss = \"mean_squared_error\" , metrics = None , dropout_rate = None , ** kwargs ) Regression Dense layers. The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be single-column or multi-column. The values should all be numerical. Arguments output_dim : Int. The number of output dimensions. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use mean_squared_error . metrics : A list of Keras metrics. Defaults to use mean_squared_error . dropout_rate : Float. The dropout rate for the layers. If left unspecified, it will be tuned automatically.","title":"RegressionHead class"},{"location":"image_classifier/","text":"[source] ImageClassifier class autokeras . ImageClassifier ( num_classes : Optional [ int ] = None , multi_label : bool = False , loss : Union [ str , Callable , None ] = None , metrics : Optional [ List [ Union [ str , Callable ]]] = None , name : str = \"image_classifier\" , max_trials : int = 100 , directory : Union [ str , pathlib . Path , None ] = None , objective : str = \"val_loss\" , overwrite : bool = True , seed : Optional [ int ] = None , ) AutoKeras image classification class. Arguments num_classes : Int. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use 'binary_crossentropy' or 'categorical_crossentropy' based on the number of classes. metrics : A list of Keras metrics. Defaults to use 'accuracy'. name : String. The name of the AutoModel. Defaults to 'image_classifier'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. overwrite : Boolean. Defaults to True . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source] fit method ImageClassifier . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. The shape of the data should be 3 or 4 dimensional, the last dimension of which should be channel dimension. y : numpy.ndarray or tensorflow.Dataset. Training data y. It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit. [source] predict method ImageClassifier . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Int. Number of samples per gradient update. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source] evaluate method ImageClassifier . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Int. Number of samples per gradient update. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source] export_model method ImageClassifier . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"ImageClassifier"},{"location":"image_classifier/#imageclassifier-class","text":"autokeras . ImageClassifier ( num_classes : Optional [ int ] = None , multi_label : bool = False , loss : Union [ str , Callable , None ] = None , metrics : Optional [ List [ Union [ str , Callable ]]] = None , name : str = \"image_classifier\" , max_trials : int = 100 , directory : Union [ str , pathlib . Path , None ] = None , objective : str = \"val_loss\" , overwrite : bool = True , seed : Optional [ int ] = None , ) AutoKeras image classification class. Arguments num_classes : Int. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use 'binary_crossentropy' or 'categorical_crossentropy' based on the number of classes. metrics : A list of Keras metrics. Defaults to use 'accuracy'. name : String. The name of the AutoModel. Defaults to 'image_classifier'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. overwrite : Boolean. Defaults to True . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source]","title":"ImageClassifier class"},{"location":"image_classifier/#fit-method","text":"ImageClassifier . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. The shape of the data should be 3 or 4 dimensional, the last dimension of which should be channel dimension. y : numpy.ndarray or tensorflow.Dataset. Training data y. It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit. [source]","title":"fit method"},{"location":"image_classifier/#predict-method","text":"ImageClassifier . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Int. Number of samples per gradient update. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source]","title":"predict method"},{"location":"image_classifier/#evaluate-method","text":"ImageClassifier . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Int. Number of samples per gradient update. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source]","title":"evaluate method"},{"location":"image_classifier/#export_model-method","text":"ImageClassifier . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"export_model method"},{"location":"image_regressor/","text":"[source] ImageRegressor class autokeras . ImageRegressor ( output_dim = None , loss = \"mean_squared_error\" , metrics = None , name = \"image_regressor\" , max_trials = 100 , directory = None , objective = \"val_loss\" , overwrite = True , seed = None , ) AutoKeras image regression class. Arguments output_dim : Int. The number of output dimensions. Defaults to None. If None, it will infer from the data. loss : A Keras loss function. Defaults to use 'mean_squared_error'. metrics : A list of Keras metrics. Defaults to use 'mean_squared_error'. name : String. The name of the AutoModel. Defaults to 'image_regressor'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. overwrite : Boolean. Defaults to True . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source] fit method ImageRegressor . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. The shape of the data should be 3 or 4 dimensional, the last dimension of which should be channel dimension. y : numpy.ndarray or tensorflow.Dataset. Training data y. The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be single-column or multi-column. The values should all be numerical. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit. [source] predict method ImageRegressor . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Int. Number of samples per gradient update. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source] evaluate method ImageRegressor . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Int. Number of samples per gradient update. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source] export_model method ImageRegressor . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"ImageRegressor"},{"location":"image_regressor/#imageregressor-class","text":"autokeras . ImageRegressor ( output_dim = None , loss = \"mean_squared_error\" , metrics = None , name = \"image_regressor\" , max_trials = 100 , directory = None , objective = \"val_loss\" , overwrite = True , seed = None , ) AutoKeras image regression class. Arguments output_dim : Int. The number of output dimensions. Defaults to None. If None, it will infer from the data. loss : A Keras loss function. Defaults to use 'mean_squared_error'. metrics : A list of Keras metrics. Defaults to use 'mean_squared_error'. name : String. The name of the AutoModel. Defaults to 'image_regressor'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. overwrite : Boolean. Defaults to True . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source]","title":"ImageRegressor class"},{"location":"image_regressor/#fit-method","text":"ImageRegressor . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. The shape of the data should be 3 or 4 dimensional, the last dimension of which should be channel dimension. y : numpy.ndarray or tensorflow.Dataset. Training data y. The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be single-column or multi-column. The values should all be numerical. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit. [source]","title":"fit method"},{"location":"image_regressor/#predict-method","text":"ImageRegressor . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Int. Number of samples per gradient update. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source]","title":"predict method"},{"location":"image_regressor/#evaluate-method","text":"ImageRegressor . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Int. Number of samples per gradient update. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source]","title":"evaluate method"},{"location":"image_regressor/#export_model-method","text":"ImageRegressor . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"export_model method"},{"location":"install/","text":"Requirements Python 3 : Follow the TensorFlow install steps to install Python 3. Pip : Follow the TensorFlow install steps to install Pip. Tensorflow >= 2.1.0 : AutoKeras is based on TensorFlow. Please follow this tutorial to install TensorFlow for python3. GPU Setup (Optional) : If you have GPUs on your machine and want to use them to accelerate the training, you can follow this tutorial to setup. Install AutoKeras AutoKeras only support Python 3 . If you followed previous steps to use virtualenv to install tensorflow, you can just activate the virtualenv and use the following command to install AutoKeras. pip install autokeras If you did not use virtualenv, and you use python3 command to execute your python program, please use the following command to install AutoKeras. python3 - m pip install autokeras","title":"Installation"},{"location":"install/#requirements","text":"Python 3 : Follow the TensorFlow install steps to install Python 3. Pip : Follow the TensorFlow install steps to install Pip. Tensorflow >= 2.1.0 : AutoKeras is based on TensorFlow. Please follow this tutorial to install TensorFlow for python3. GPU Setup (Optional) : If you have GPUs on your machine and want to use them to accelerate the training, you can follow this tutorial to setup.","title":"Requirements"},{"location":"install/#install-autokeras","text":"AutoKeras only support Python 3 . If you followed previous steps to use virtualenv to install tensorflow, you can just activate the virtualenv and use the following command to install AutoKeras. pip install autokeras If you did not use virtualenv, and you use python3 command to execute your python program, please use the following command to install AutoKeras. python3 - m pip install autokeras","title":"Install AutoKeras"},{"location":"node/","text":"[source] ImageInput class autokeras . ImageInput ( shape = None ) Input node for image data. The input data should be numpy.ndarray or tf.data.Dataset. The shape of the data should be 3 or 4 dimensional, the last dimension of which should be channel dimension. [source] Input class autokeras . Input ( shape = None ) Input node for tensor data. The data should be numpy.ndarray or tf.data.Dataset. [source] StructuredDataInput class autokeras . StructuredDataInput ( column_names = None , column_types = None , ** kwargs ) Input node for structured data. The input data should be numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. The data should be two-dimensional with numerical or categorical values. Arguments column_names : A list of strings specifying the names of the columns. The length of the list should be equal to the number of columns of the data. Defaults to None. If None, it will be obtained from the header of the csv file or the pandas.DataFrame. column_types : Dict. The keys are the column names. The values should either be 'numerical' or 'categorical', indicating the type of that column. Defaults to None. If not None, the column_names need to be specified. If None, it will be inferred from the data. A column will be judged as categorical if the number of different values is less than 5% of the number of instances. [source] TextInput class autokeras . TextInput ( shape = None ) Input node for text data. The input data should be numpy.ndarray or tf.data.Dataset. The data should be one-dimensional. Each element in the data should be a string which is a full sentence.","title":"Node"},{"location":"node/#imageinput-class","text":"autokeras . ImageInput ( shape = None ) Input node for image data. The input data should be numpy.ndarray or tf.data.Dataset. The shape of the data should be 3 or 4 dimensional, the last dimension of which should be channel dimension. [source]","title":"ImageInput class"},{"location":"node/#input-class","text":"autokeras . Input ( shape = None ) Input node for tensor data. The data should be numpy.ndarray or tf.data.Dataset. [source]","title":"Input class"},{"location":"node/#structureddatainput-class","text":"autokeras . StructuredDataInput ( column_names = None , column_types = None , ** kwargs ) Input node for structured data. The input data should be numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. The data should be two-dimensional with numerical or categorical values. Arguments column_names : A list of strings specifying the names of the columns. The length of the list should be equal to the number of columns of the data. Defaults to None. If None, it will be obtained from the header of the csv file or the pandas.DataFrame. column_types : Dict. The keys are the column names. The values should either be 'numerical' or 'categorical', indicating the type of that column. Defaults to None. If not None, the column_names need to be specified. If None, it will be inferred from the data. A column will be judged as categorical if the number of different values is less than 5% of the number of instances. [source]","title":"StructuredDataInput class"},{"location":"node/#textinput-class","text":"autokeras . TextInput ( shape = None ) Input node for text data. The input data should be numpy.ndarray or tf.data.Dataset. The data should be one-dimensional. Each element in the data should be a string which is a full sentence.","title":"TextInput class"},{"location":"preprocessor/","text":"[source] ImageAugmentation class autokeras . ImageAugmentation ( percentage = 0.25 , rotation_range = 180 , random_crop = True , brightness_range = 0.5 , saturation_range = 0.5 , contrast_range = 0.5 , translation = True , horizontal_flip = True , vertical_flip = True , gaussian_noise = True , ** kwargs ) Collection of various image augmentation methods. Arguments percentage : Float. The percentage of data to augment. rotation_range : Int. The value can only be 0, 90, or 180. Degree range for random rotations. Default to 180. random_crop : Boolean. Whether to crop the image randomly. Default to True. brightness_range : Positive float. Serve as 'max_delta' in tf.image.random_brightness. Default to 0.5. Equivalent to adjust brightness using a 'delta' randomly picked in the interval [-max_delta, max_delta). saturation_range : Positive float or Tuple. If given a positive float, _get_min_and_max() will automated generate a tuple for saturation range. If given a tuple directly, it will serve as a range for picking a saturation shift value from. Default to 0.5. contrast_range : Positive float or Tuple. If given a positive float, _get_min_and_max() will automated generate a tuple for contrast range. If given a tuple directly, it will serve as a range for picking a contrast shift value from. Default to 0.5. translation : Boolean. Whether to translate the image. horizontal_flip : Boolean. Whether to flip the image horizontally. vertical_flip : Boolean. Whether to flip the image vertically. gaussian_noise : Boolean. Whether to add gaussian noise to the image. [source] Normalization class autokeras . Normalization ( axis : int = - 1 , ** kwargs ) Perform basic image transformation and augmentation. Arguments axis : Integer or tuple of integers, the axis or axes that should be normalized (typically the features axis). We will normalize each element in the specified axis. The default is '-1' (the innermost axis); 0 (the batch axis) is not allowed. [source] TextToIntSequence class autokeras . TextToIntSequence ( output_sequence_length = None , max_tokens = 20000 , ** kwargs ) Convert raw texts to sequences of word indices. Arguments output_sequence_length : Int. The maximum length of a sentence. If unspecified, it would be tuned automatically. max_tokens : Int. The maximum size of the vocabulary. Defaults to 20000. [source] TextToNgramVector class autokeras . TextToNgramVector ( max_tokens = 20000 , ** kwargs ) Convert raw texts to n-gram vectors. Arguments max_tokens : Int. The maximum size of the vocabulary. Defaults to 20000. [source] CategoricalToNumerical class autokeras . CategoricalToNumerical ( ** kwargs ) Encode the categorical features to numerical features.","title":"Preprocessor"},{"location":"preprocessor/#imageaugmentation-class","text":"autokeras . ImageAugmentation ( percentage = 0.25 , rotation_range = 180 , random_crop = True , brightness_range = 0.5 , saturation_range = 0.5 , contrast_range = 0.5 , translation = True , horizontal_flip = True , vertical_flip = True , gaussian_noise = True , ** kwargs ) Collection of various image augmentation methods. Arguments percentage : Float. The percentage of data to augment. rotation_range : Int. The value can only be 0, 90, or 180. Degree range for random rotations. Default to 180. random_crop : Boolean. Whether to crop the image randomly. Default to True. brightness_range : Positive float. Serve as 'max_delta' in tf.image.random_brightness. Default to 0.5. Equivalent to adjust brightness using a 'delta' randomly picked in the interval [-max_delta, max_delta). saturation_range : Positive float or Tuple. If given a positive float, _get_min_and_max() will automated generate a tuple for saturation range. If given a tuple directly, it will serve as a range for picking a saturation shift value from. Default to 0.5. contrast_range : Positive float or Tuple. If given a positive float, _get_min_and_max() will automated generate a tuple for contrast range. If given a tuple directly, it will serve as a range for picking a contrast shift value from. Default to 0.5. translation : Boolean. Whether to translate the image. horizontal_flip : Boolean. Whether to flip the image horizontally. vertical_flip : Boolean. Whether to flip the image vertically. gaussian_noise : Boolean. Whether to add gaussian noise to the image. [source]","title":"ImageAugmentation class"},{"location":"preprocessor/#normalization-class","text":"autokeras . Normalization ( axis : int = - 1 , ** kwargs ) Perform basic image transformation and augmentation. Arguments axis : Integer or tuple of integers, the axis or axes that should be normalized (typically the features axis). We will normalize each element in the specified axis. The default is '-1' (the innermost axis); 0 (the batch axis) is not allowed. [source]","title":"Normalization class"},{"location":"preprocessor/#texttointsequence-class","text":"autokeras . TextToIntSequence ( output_sequence_length = None , max_tokens = 20000 , ** kwargs ) Convert raw texts to sequences of word indices. Arguments output_sequence_length : Int. The maximum length of a sentence. If unspecified, it would be tuned automatically. max_tokens : Int. The maximum size of the vocabulary. Defaults to 20000. [source]","title":"TextToIntSequence class"},{"location":"preprocessor/#texttongramvector-class","text":"autokeras . TextToNgramVector ( max_tokens = 20000 , ** kwargs ) Convert raw texts to n-gram vectors. Arguments max_tokens : Int. The maximum size of the vocabulary. Defaults to 20000. [source]","title":"TextToNgramVector class"},{"location":"preprocessor/#categoricaltonumerical-class","text":"autokeras . CategoricalToNumerical ( ** kwargs ) Encode the categorical features to numerical features.","title":"CategoricalToNumerical class"},{"location":"structured_data_classifier/","text":"[source] StructuredDataClassifier class autokeras . StructuredDataClassifier ( column_names = None , column_types = None , num_classes = None , multi_label = False , loss = None , metrics = None , name = \"structured_data_classifier\" , max_trials = 100 , directory = None , objective = \"val_accuracy\" , overwrite = True , seed = None , ) AutoKeras structured data classification class. Arguments column_names : A list of strings specifying the names of the columns. The length of the list should be equal to the number of columns of the data excluding the target column. Defaults to None. If None, it will obtained from the header of the csv file or the pandas.DataFrame. column_types : Dict. The keys are the column names. The values should either be 'numerical' or 'categorical', indicating the type of that column. Defaults to None. If not None, the column_names need to be specified. If None, it will be inferred from the data. num_classes : Int. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use 'binary_crossentropy' or 'categorical_crossentropy' based on the number of classes. metrics : A list of Keras metrics. Defaults to use 'accuracy'. name : String. The name of the AutoModel. Defaults to 'structured_data_classifier'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize. Defaults to 'val_accuracy'. overwrite : Boolean. Defaults to True . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source] fit method StructuredDataClassifier . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the task. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Training data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the training data. y : String, numpy.ndarray, or tensorflow.Dataset. Training data y. If the data is from a csv file, it should be a string, which is the name of the target column. Otherwise, It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification. epochs : Int. The number of epochs to train each model during the search. If unspecified, we would use epochs equal to 1000 and early stopping with patience equal to 30. callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. **kwargs : Any arguments supported by keras.Model.fit. [source] predict method StructuredDataClassifier . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Testing data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the testing data. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source] evaluate method StructuredDataClassifier . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Testing data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the testing data. y : String, numpy.ndarray, or tensorflow.Dataset. Testing data y. If the data is from a csv file, it should be a string corresponding to the label column. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source] export_model method StructuredDataClassifier . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"StructuredDataClassifier"},{"location":"structured_data_classifier/#structureddataclassifier-class","text":"autokeras . StructuredDataClassifier ( column_names = None , column_types = None , num_classes = None , multi_label = False , loss = None , metrics = None , name = \"structured_data_classifier\" , max_trials = 100 , directory = None , objective = \"val_accuracy\" , overwrite = True , seed = None , ) AutoKeras structured data classification class. Arguments column_names : A list of strings specifying the names of the columns. The length of the list should be equal to the number of columns of the data excluding the target column. Defaults to None. If None, it will obtained from the header of the csv file or the pandas.DataFrame. column_types : Dict. The keys are the column names. The values should either be 'numerical' or 'categorical', indicating the type of that column. Defaults to None. If not None, the column_names need to be specified. If None, it will be inferred from the data. num_classes : Int. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use 'binary_crossentropy' or 'categorical_crossentropy' based on the number of classes. metrics : A list of Keras metrics. Defaults to use 'accuracy'. name : String. The name of the AutoModel. Defaults to 'structured_data_classifier'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize. Defaults to 'val_accuracy'. overwrite : Boolean. Defaults to True . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source]","title":"StructuredDataClassifier class"},{"location":"structured_data_classifier/#fit-method","text":"StructuredDataClassifier . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the task. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Training data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the training data. y : String, numpy.ndarray, or tensorflow.Dataset. Training data y. If the data is from a csv file, it should be a string, which is the name of the target column. Otherwise, It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification. epochs : Int. The number of epochs to train each model during the search. If unspecified, we would use epochs equal to 1000 and early stopping with patience equal to 30. callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. **kwargs : Any arguments supported by keras.Model.fit. [source]","title":"fit method"},{"location":"structured_data_classifier/#predict-method","text":"StructuredDataClassifier . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Testing data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the testing data. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source]","title":"predict method"},{"location":"structured_data_classifier/#evaluate-method","text":"StructuredDataClassifier . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Testing data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the testing data. y : String, numpy.ndarray, or tensorflow.Dataset. Testing data y. If the data is from a csv file, it should be a string corresponding to the label column. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source]","title":"evaluate method"},{"location":"structured_data_classifier/#export_model-method","text":"StructuredDataClassifier . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"export_model method"},{"location":"structured_data_regressor/","text":"[source] StructuredDataRegressor class autokeras . StructuredDataRegressor ( column_names = None , column_types = None , output_dim = None , loss = \"mean_squared_error\" , metrics = None , name = \"structured_data_regressor\" , max_trials = 100 , directory = None , objective = \"val_loss\" , overwrite = True , seed = None , ) AutoKeras structured data regression class. Arguments column_names : A list of strings specifying the names of the columns. The length of the list should be equal to the number of columns of the data excluding the target column. Defaults to None. If None, it will obtained from the header of the csv file or the pandas.DataFrame. column_types : Dict. The keys are the column names. The values should either be 'numerical' or 'categorical', indicating the type of that column. Defaults to None. If not None, the column_names need to be specified. If None, it will be inferred from the data. output_dim : Int. The number of output dimensions. Defaults to None. If None, it will infer from the data. loss : A Keras loss function. Defaults to use 'mean_squared_error'. metrics : A list of Keras metrics. Defaults to use 'mean_squared_error'. name : String. The name of the AutoModel. Defaults to 'structured_data_regressor'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. overwrite : Boolean. Defaults to True . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source] fit method StructuredDataRegressor . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the task. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Training data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the training data. y : String, numpy.ndarray, or tensorflow.Dataset. Training data y. If the data is from a csv file, it should be a string, which is the name of the target column. Otherwise, it can be single-column or multi-column. The values should all be numerical. epochs : Int. The number of epochs to train each model during the search. If unspecified, we would use epochs equal to 1000 and early stopping with patience equal to 30. callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit. [source] predict method StructuredDataRegressor . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Testing data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the testing data. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source] evaluate method StructuredDataRegressor . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Testing data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the testing data. y : String, numpy.ndarray, or tensorflow.Dataset. Testing data y. If the data is from a csv file, it should be a string corresponding to the label column. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source] export_model method StructuredDataRegressor . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"StructuredDataRegressor"},{"location":"structured_data_regressor/#structureddataregressor-class","text":"autokeras . StructuredDataRegressor ( column_names = None , column_types = None , output_dim = None , loss = \"mean_squared_error\" , metrics = None , name = \"structured_data_regressor\" , max_trials = 100 , directory = None , objective = \"val_loss\" , overwrite = True , seed = None , ) AutoKeras structured data regression class. Arguments column_names : A list of strings specifying the names of the columns. The length of the list should be equal to the number of columns of the data excluding the target column. Defaults to None. If None, it will obtained from the header of the csv file or the pandas.DataFrame. column_types : Dict. The keys are the column names. The values should either be 'numerical' or 'categorical', indicating the type of that column. Defaults to None. If not None, the column_names need to be specified. If None, it will be inferred from the data. output_dim : Int. The number of output dimensions. Defaults to None. If None, it will infer from the data. loss : A Keras loss function. Defaults to use 'mean_squared_error'. metrics : A list of Keras metrics. Defaults to use 'mean_squared_error'. name : String. The name of the AutoModel. Defaults to 'structured_data_regressor'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. overwrite : Boolean. Defaults to True . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source]","title":"StructuredDataRegressor class"},{"location":"structured_data_regressor/#fit-method","text":"StructuredDataRegressor . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the task. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Training data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the training data. y : String, numpy.ndarray, or tensorflow.Dataset. Training data y. If the data is from a csv file, it should be a string, which is the name of the target column. Otherwise, it can be single-column or multi-column. The values should all be numerical. epochs : Int. The number of epochs to train each model during the search. If unspecified, we would use epochs equal to 1000 and early stopping with patience equal to 30. callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit. [source]","title":"fit method"},{"location":"structured_data_regressor/#predict-method","text":"StructuredDataRegressor . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Testing data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the testing data. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source]","title":"predict method"},{"location":"structured_data_regressor/#evaluate-method","text":"StructuredDataRegressor . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Testing data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the testing data. y : String, numpy.ndarray, or tensorflow.Dataset. Testing data y. If the data is from a csv file, it should be a string corresponding to the label column. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source]","title":"evaluate method"},{"location":"structured_data_regressor/#export_model-method","text":"StructuredDataRegressor . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"export_model method"},{"location":"text_classifier/","text":"[source] TextClassifier class autokeras . TextClassifier ( num_classes : Optional [ int ] = None , multi_label : bool = False , loss : Union [ str , Callable , None ] = None , metrics : Union [ List [ Union [ str , Callable , None ]], List [ List [ Union [ str , Callable , None ]]], Dict [ str , Union [ str , Callable , None ]], None , ] = None , name : str = \"text_classifier\" , max_trials : int = 100 , directory : Union [ str , pathlib . Path , None ] = None , objective : str = \"val_loss\" , overwrite : bool = True , seed : Optional [ int ] = None , ) AutoKeras text classification class. Arguments num_classes : Int. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use 'binary_crossentropy' or 'categorical_crossentropy' based on the number of classes. metrics : A list of Keras metrics. Defaults to use 'accuracy'. name : String. The name of the AutoModel. Defaults to 'text_classifier'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. overwrite : Boolean. Defaults to True . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source] fit method TextClassifier . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. The input data should be numpy.ndarray or tf.data.Dataset. The data should be one dimensional. Each element in the data should be a string which is a full sentence. y : numpy.ndarray or tensorflow.Dataset. Training data y. It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit. [source] predict method TextClassifier . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Int. Number of samples per gradient update. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source] evaluate method TextClassifier . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Int. Number of samples per gradient update. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source] export_model method TextClassifier . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"TextClassifier"},{"location":"text_classifier/#textclassifier-class","text":"autokeras . TextClassifier ( num_classes : Optional [ int ] = None , multi_label : bool = False , loss : Union [ str , Callable , None ] = None , metrics : Union [ List [ Union [ str , Callable , None ]], List [ List [ Union [ str , Callable , None ]]], Dict [ str , Union [ str , Callable , None ]], None , ] = None , name : str = \"text_classifier\" , max_trials : int = 100 , directory : Union [ str , pathlib . Path , None ] = None , objective : str = \"val_loss\" , overwrite : bool = True , seed : Optional [ int ] = None , ) AutoKeras text classification class. Arguments num_classes : Int. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use 'binary_crossentropy' or 'categorical_crossentropy' based on the number of classes. metrics : A list of Keras metrics. Defaults to use 'accuracy'. name : String. The name of the AutoModel. Defaults to 'text_classifier'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. overwrite : Boolean. Defaults to True . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source]","title":"TextClassifier class"},{"location":"text_classifier/#fit-method","text":"TextClassifier . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. The input data should be numpy.ndarray or tf.data.Dataset. The data should be one dimensional. Each element in the data should be a string which is a full sentence. y : numpy.ndarray or tensorflow.Dataset. Training data y. It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit. [source]","title":"fit method"},{"location":"text_classifier/#predict-method","text":"TextClassifier . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Int. Number of samples per gradient update. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source]","title":"predict method"},{"location":"text_classifier/#evaluate-method","text":"TextClassifier . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Int. Number of samples per gradient update. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source]","title":"evaluate method"},{"location":"text_classifier/#export_model-method","text":"TextClassifier . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"export_model method"},{"location":"text_regressor/","text":"[source] TextRegressor class autokeras . TextRegressor ( output_dim = None , loss = \"mean_squared_error\" , metrics = None , name = \"text_regressor\" , max_trials = 100 , directory = None , objective = \"val_loss\" , overwrite = True , seed = None , ) AutoKeras text regression class. Arguments output_dim : Int. The number of output dimensions. Defaults to None. If None, it will infer from the data. loss : A Keras loss function. Defaults to use 'mean_squared_error'. metrics : A list of Keras metrics. Defaults to use 'mean_squared_error'. name : String. The name of the AutoModel. Defaults to 'text_regressor'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. overwrite : Boolean. Defaults to True . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source] fit method TextRegressor . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. The input data should be numpy.ndarray or tf.data.Dataset. The data should be one dimensional. Each element in the data should be a string which is a full sentence. y : numpy.ndarray or tensorflow.Dataset. Training data y. The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be single-column or multi-column. The values should all be numerical. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit. [source] predict method TextRegressor . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Int. Number of samples per gradient update. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source] evaluate method TextRegressor . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Int. Number of samples per gradient update. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source] export_model method TextRegressor . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"TextRegressor"},{"location":"text_regressor/#textregressor-class","text":"autokeras . TextRegressor ( output_dim = None , loss = \"mean_squared_error\" , metrics = None , name = \"text_regressor\" , max_trials = 100 , directory = None , objective = \"val_loss\" , overwrite = True , seed = None , ) AutoKeras text regression class. Arguments output_dim : Int. The number of output dimensions. Defaults to None. If None, it will infer from the data. loss : A Keras loss function. Defaults to use 'mean_squared_error'. metrics : A list of Keras metrics. Defaults to use 'mean_squared_error'. name : String. The name of the AutoModel. Defaults to 'text_regressor'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. overwrite : Boolean. Defaults to True . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed : Int. Random seed. [source]","title":"TextRegressor class"},{"location":"text_regressor/#fit-method","text":"TextRegressor . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. The input data should be numpy.ndarray or tf.data.Dataset. The data should be one dimensional. Each element in the data should be a string which is a full sentence. y : numpy.ndarray or tensorflow.Dataset. Training data y. The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be single-column or multi-column. The values should all be numerical. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit. [source]","title":"fit method"},{"location":"text_regressor/#predict-method","text":"TextRegressor . predict ( x , batch_size = 32 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Int. Number of samples per gradient update. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source]","title":"predict method"},{"location":"text_regressor/#evaluate-method","text":"TextRegressor . evaluate ( x , y = None , batch_size = 32 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Int. Number of samples per gradient update. Defaults to 32. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source]","title":"evaluate method"},{"location":"text_regressor/#export_model-method","text":"TextRegressor . export_model () Export the best Keras Model. Returns tf.keras.Model instance. The best model found during the search, loaded with trained weights.","title":"export_model method"},{"location":"examples/imdb/","text":"Search for a good model for the IMDB dataset. import numpy as np import tensorflow as tf import autokeras as ak def imdb_raw (): max_features = 20000 index_offset = 3 # word index offset ( x_train , y_train ), ( x_test , y_test ) = tf . keras . datasets . imdb . load_data ( num_words = max_features , index_from = index_offset ) x_train = x_train y_train = y_train . reshape ( - 1 , 1 ) x_test = x_test y_test = y_test . reshape ( - 1 , 1 ) word_to_id = tf . keras . datasets . imdb . get_word_index () word_to_id = { k : ( v + index_offset ) for k , v in word_to_id . items ()} word_to_id [ \"<PAD>\" ] = 0 word_to_id [ \"<START>\" ] = 1 word_to_id [ \"<UNK>\" ] = 2 id_to_word = { value : key for key , value in word_to_id . items ()} x_train = list ( map ( lambda sentence : ' ' . join ( id_to_word [ i ] for i in sentence ), x_train )) x_test = list ( map ( lambda sentence : ' ' . join ( id_to_word [ i ] for i in sentence ), x_test )) x_train = np . array ( x_train , dtype = np . str ) x_test = np . array ( x_test , dtype = np . str ) return ( x_train , y_train ), ( x_test , y_test ) # Prepare the data. ( x_train , y_train ), ( x_test , y_test ) = imdb_raw () print ( x_train . shape ) # (25000,) print ( y_train . shape ) # (25000, 1) print ( x_train [ 0 ][: 50 ]) # <START> this film was just brilliant casting <UNK> # Initialize the TextClassifier clf = ak . TextClassifier ( max_trials = 3 ) # Search for the best model. clf . fit ( x_train , y_train , epochs = 2 ) # Evaluate on the testing data. print ( 'Accuracy: {accuracy}' . format ( clf . evaluate ( x_test , y_test )))","title":"IMDB Movie Reviews"},{"location":"examples/mnist/","text":"Search for a good model for the MNIST dataset. from tensorflow.keras.datasets import mnist import autokeras as ak # Prepare the dataset. ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () print ( x_train . shape ) # (60000, 28, 28) print ( y_train . shape ) # (60000,) print ( y_train [: 3 ]) # array([7, 2, 1], dtype=uint8) # Initialize the ImageClassifier. clf = ak . ImageClassifier ( max_trials = 3 ) # Search for the best model. clf . fit ( x_train , y_train ) # Evaluate on the testing data. print ( 'Accuracy: {accuracy}' . format ( accuracy = clf . evaluate ( x_test , y_test )))","title":"MNIST Hand-Written Digits"},{"location":"examples/titanic/","text":"Search for a good model for the Titanic dataset. First, you need to download the titanic dataset file train.csv and eval.csv . Second, replace PATH_TO/train.csv and PATH_TO/eval.csv in the following example with the real path to those two files. Then, you can run the code. import autokeras as ak # Initialize the classifier. clf = ak . StructuredDataClassifier ( max_trials = 30 ) # x is the path to the csv file. y is the column name of the column to predict. clf . fit ( x = 'PATH_TO/train.csv' , y = 'survived' ) # Evaluate the accuracy of the found model. print ( 'Accuracy: {accuracy}' . format ( accuracy = clf . evaluate ( x = 'PATH_TO/eval.csv' , y = 'survived' )))","title":"Titanic Survival Prediction"},{"location":"tutorial/customized/","text":"In this tutorial, we show how to customize your search space with AutoModel and how to implement your own block as search space. This API is mainly for advanced users who already know what their model should look like. Customized Search Space First, let us see how we can build the following neural network using the building blocks in AutoKeras. graph LR id1(ImageInput) --> id2(Normalization) id2 --> id3(Image Augmentation) id3 --> id4(Convolutional) id3 --> id5(ResNet V2) id4 --> id6(Merge) id5 --> id6 id6 --> id7(Classification Head) We can make use of the AutoModel API in AutoKeras to implemented as follows. The usage is the same as the Keras functional API . import autokeras as ak input_node = ak . ImageInput () output_node = ak . Normalization ()( input_node ) output_node = ak . ImageAugmentation ()( output_node ) output_node1 = ak . ConvBlock ()( output_node ) output_node2 = ak . ResNetBlock ( version = 'v2' )( output_node ) output_node = ak . Merge ()([ output_node1 , output_node2 ]) output_node = ak . ClassificationHead ()( output_node ) auto_model = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) Whild building the model, the blocks used need to follow this topology: Preprocessor -> Block -> Head . Normalization and ImageAugmentation are Preprocessor s. ClassificationHead is Head . The rest are Block s. In the code above, we use ak.ResNetBlock(version='v2') to specify the version of ResNet to use. There are many other arguments to specify for each building block. For most of the arguments, if not specified, they would be tuned automatically. Please refer to the documentation links at the bottom of the page for more details. Then, we prepare some data to run the model. from tensorflow.keras.datasets import mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () print ( x_train . shape ) # (60000, 28, 28) print ( y_train . shape ) # (60000,) print ( y_train [: 3 ]) # array([7, 2, 1], dtype=uint8) # Feed the AutoModel with training data. auto_model . fit ( x_train , y_train ) # Predict with the best model. predicted_y = auto_model . predict ( x_test ) # Evaluate the best model with testing data. print ( auto_model . evaluate ( x_test , y_test )) For multiple input nodes and multiple heads search space, you can refer to this section . Validation Data If you would like to provide your own validation data or change the ratio of the validation data, please refer to the Validation Data section of the tutorials of Image Classification , Text Classification , Structured Data Classification , Multi-task and Multiple Validation . Data Format You can refer to the documentation of ImageInput , StructuredDataInput , TextInput , RegressionHead , ClassificationHead , for the format of different types of data. You can also refer to the Data Format section of the tutorials of Image Classification , Text Classification , Structured Data Classification . Implement New Block You can extend the Block class to implement your own building blocks and use it with AutoModel . The first step is to learn how to write a build function for KerasTuner . You need to override the build function of the block. The following example shows how to implement a single Dense layer block whose number of neurons is tunable. import autokeras as ak import tensorflow as tf class SingleDenseLayerBlock ( ak . Block ): def build ( self , hp , inputs = None ): # Get the input_node from inputs. input_node = tf . python . util . nest . flatten ( inputs )[ 0 ] layer = tf . keras . layers . Dense ( hp . Int ( 'num_units' , min_value = 32 , max_value = 512 , step = 32 )) output_node = layer ( input_node ) return output_node You can connect it with other blocks and build it into an AutoModel . # Build the AutoModel input_node = ak . Input () output_node = SingleDenseLayerBlock ()( input_node ) output_node = ak . RegressionHead ()( output_node ) auto_model = ak . AutoModel ( input_node , output_node , max_trials = 10 ) # Prepare Data num_instances = 100 x_train = np . random . rand ( num_instances , 20 ) . astype ( np . float32 ) y_train = np . random . rand ( num_instances , 1 ) . astype ( np . float32 ) x_test = np . random . rand ( num_instances , 20 ) . astype ( np . float32 ) y_test = np . random . rand ( num_instances , 1 ) . astype ( np . float32 ) # Train the model auto_model . fit ( x_train , y_train ) print ( auto_model . evaluate ( x_test , y_test )) Reference AutoModel Nodes : ImageInput , Input , StructuredDataInput , TextInput . Preprocessors : FeatureEngineering , ImageAugmentation , LightGBM , Normalization , TextToIntSequence , TextToNgramVector . Blocks : ConvBlock , DenseBlock , Embedding , Merge , ResNetBlock , RNNBlock , SpatialReduction , TemporalReduction , XceptionBlock , ImageBlock , StructuredDataBlock , TextBlock .","title":"Customized Model"},{"location":"tutorial/customized/#customized-search-space","text":"First, let us see how we can build the following neural network using the building blocks in AutoKeras. graph LR id1(ImageInput) --> id2(Normalization) id2 --> id3(Image Augmentation) id3 --> id4(Convolutional) id3 --> id5(ResNet V2) id4 --> id6(Merge) id5 --> id6 id6 --> id7(Classification Head) We can make use of the AutoModel API in AutoKeras to implemented as follows. The usage is the same as the Keras functional API . import autokeras as ak input_node = ak . ImageInput () output_node = ak . Normalization ()( input_node ) output_node = ak . ImageAugmentation ()( output_node ) output_node1 = ak . ConvBlock ()( output_node ) output_node2 = ak . ResNetBlock ( version = 'v2' )( output_node ) output_node = ak . Merge ()([ output_node1 , output_node2 ]) output_node = ak . ClassificationHead ()( output_node ) auto_model = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) Whild building the model, the blocks used need to follow this topology: Preprocessor -> Block -> Head . Normalization and ImageAugmentation are Preprocessor s. ClassificationHead is Head . The rest are Block s. In the code above, we use ak.ResNetBlock(version='v2') to specify the version of ResNet to use. There are many other arguments to specify for each building block. For most of the arguments, if not specified, they would be tuned automatically. Please refer to the documentation links at the bottom of the page for more details. Then, we prepare some data to run the model. from tensorflow.keras.datasets import mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () print ( x_train . shape ) # (60000, 28, 28) print ( y_train . shape ) # (60000,) print ( y_train [: 3 ]) # array([7, 2, 1], dtype=uint8) # Feed the AutoModel with training data. auto_model . fit ( x_train , y_train ) # Predict with the best model. predicted_y = auto_model . predict ( x_test ) # Evaluate the best model with testing data. print ( auto_model . evaluate ( x_test , y_test )) For multiple input nodes and multiple heads search space, you can refer to this section .","title":"Customized Search Space"},{"location":"tutorial/customized/#validation-data","text":"If you would like to provide your own validation data or change the ratio of the validation data, please refer to the Validation Data section of the tutorials of Image Classification , Text Classification , Structured Data Classification , Multi-task and Multiple Validation .","title":"Validation Data"},{"location":"tutorial/customized/#data-format","text":"You can refer to the documentation of ImageInput , StructuredDataInput , TextInput , RegressionHead , ClassificationHead , for the format of different types of data. You can also refer to the Data Format section of the tutorials of Image Classification , Text Classification , Structured Data Classification .","title":"Data Format"},{"location":"tutorial/customized/#implement-new-block","text":"You can extend the Block class to implement your own building blocks and use it with AutoModel . The first step is to learn how to write a build function for KerasTuner . You need to override the build function of the block. The following example shows how to implement a single Dense layer block whose number of neurons is tunable. import autokeras as ak import tensorflow as tf class SingleDenseLayerBlock ( ak . Block ): def build ( self , hp , inputs = None ): # Get the input_node from inputs. input_node = tf . python . util . nest . flatten ( inputs )[ 0 ] layer = tf . keras . layers . Dense ( hp . Int ( 'num_units' , min_value = 32 , max_value = 512 , step = 32 )) output_node = layer ( input_node ) return output_node You can connect it with other blocks and build it into an AutoModel . # Build the AutoModel input_node = ak . Input () output_node = SingleDenseLayerBlock ()( input_node ) output_node = ak . RegressionHead ()( output_node ) auto_model = ak . AutoModel ( input_node , output_node , max_trials = 10 ) # Prepare Data num_instances = 100 x_train = np . random . rand ( num_instances , 20 ) . astype ( np . float32 ) y_train = np . random . rand ( num_instances , 1 ) . astype ( np . float32 ) x_test = np . random . rand ( num_instances , 20 ) . astype ( np . float32 ) y_test = np . random . rand ( num_instances , 1 ) . astype ( np . float32 ) # Train the model auto_model . fit ( x_train , y_train ) print ( auto_model . evaluate ( x_test , y_test ))","title":"Implement New Block"},{"location":"tutorial/customized/#reference","text":"AutoModel Nodes : ImageInput , Input , StructuredDataInput , TextInput . Preprocessors : FeatureEngineering , ImageAugmentation , LightGBM , Normalization , TextToIntSequence , TextToNgramVector . Blocks : ConvBlock , DenseBlock , Embedding , Merge , ResNetBlock , RNNBlock , SpatialReduction , TemporalReduction , XceptionBlock , ImageBlock , StructuredDataBlock , TextBlock .","title":"Reference"},{"location":"tutorial/export/","text":"You can easily export your model the best model found by AutoKeras as a Keras Model. The following example uses ImageClassifier as an example. All the tasks and the AutoModel has this export_model function. from tensorflow.keras.datasets import mnist import autokeras as ak ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () # Initialize the image classifier. clf = ak . ImageClassifier ( max_trials = 10 ) # It tries 10 different models. # Feed the image classifier with training data. clf . fit ( x_train , y_train ) # Export as a Keras Model. model = clf . export_model () print ( type ( model )) # <class 'tensorflow.python.keras.engine.training.Model'> To save and reload Keras Model, you need: load_model ( path , custom_objects = ak . CUSTOM_OBJECTS )","title":"Export Model"},{"location":"tutorial/faq/","text":"How to resume a previously killed fit? Feature supported. Answer in progress. How to customize metrics and loss? Feature supported. Answer in progress.","title":"FAQ"},{"location":"tutorial/faq/#how-to-resume-a-previously-killed-fit","text":"Feature supported. Answer in progress.","title":"How to resume a previously killed fit?"},{"location":"tutorial/faq/#how-to-customize-metrics-and-loss","text":"Feature supported. Answer in progress.","title":"How to customize metrics and loss?"},{"location":"tutorial/image_classification/","text":"Image Classification A Simple Example The first step is to prepare your data. Here we use the MNIST dataset as an example. from tensorflow.keras.datasets import mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () print ( x_train . shape ) # (60000, 28, 28) print ( y_train . shape ) # (60000,) print ( y_train [: 3 ]) # array([7, 2, 1], dtype=uint8) The second step is to run the ImageClassifier . import autokeras as ak # Initialize the image classifier. clf = ak . ImageClassifier ( max_trials = 10 ) # It tries 10 different models. # Feed the image classifier with training data. clf . fit ( x_train , y_train ) # Predict with the best model. predicted_y = clf . predict ( x_test ) # Evaluate the best model with testing data. print ( clf . evaluate ( x_test , y_test )) Validation Data By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. clf . fit ( x_train , y_train , # Split the training data and use the last 15% as validation data. validation_split = 0.15 ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 50000 x_val = x_train [ split :] y_val = y_train [ split :] x_train = x_train [: split ] y_train = y_train [: split ] clf . fit ( x_train , y_train , # Use your own validation set. validation_data = ( x_val , y_val )) Customized Search Space For advanced users, you may customize your search space by using AutoModel instead of ImageClassifier . You can configure the ImageBlock for some high-level configurations, e.g., block_type for the type of neural network to search, normalize for whether to do data normalization, augment for whether to do data augmentation. You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail. import autokeras as ak input_node = ak . ImageInput () output_node = ak . ImageBlock ( # Only search ResNet architectures. block_type = 'resnet' , # Normalize the dataset. normalize = True , # Do not do data augmentation. augment = False )( input_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) clf . fit ( x_train , y_train ) The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak.[some_block]([block_args])(input_node) . You can even also use more fine grained blocks to customize the search space even further. See the following example. import autokeras as ak input_node = ak . ImageInput () output_node = ak . Normalization ()( input_node ) output_node = ak . ImageAugmentation ( percentage = 0.3 )( output_node ) output_node = ak . ResNetBlock ( version = 'v2' )( output_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) clf . fit ( x_train , y_train ) Data Format The AutoKeras ImageClassifier is quite flexible for the data format. For the image, it accepts data formats both with and without the channel dimension. The images in the MNIST dataset do not have the channel dimension. Each image is a matrix with shape (28, 28). AutoKeras also accepts images of three dimensions with the channel dimension at last, e.g., (32, 32, 3), (28, 28, 1). For the classification labels, AutoKeras accepts both plain labels, i.e. strings or integers, and one-hot encoded encoded labels, i.e. vectors of 0s and 1s. So if you prepare your data in the following way, the ImageClassifier should still work. from tensorflow.keras.datasets import mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () # Reshape the images to have the channel dimension. x_train = x_train . reshape ( x_train . shape + ( 1 ,)) x_test = x_test . reshape ( x_test . shape + ( 1 ,)) # One-hot encode the labels. import numpy as np eye = np . eye ( 10 ) y_train = eye [ y_train ] y_test = eye [ y_test ] print ( x_train . shape ) # (60000, 28, 28, 1) print ( y_train . shape ) # (60000, 10) print ( y_train [: 3 ]) # array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], # [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], # [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]]) We also support using tf.data.Dataset format for the training data. In this case, the images would have to be 3-dimentional. The labels have to be one-hot encoded for multi-class classification to be wrapped into tensorflow Dataset. import tensorflow as tf train_set = tf . data . Dataset . from_tensor_slices ((( x_train , ), ( y_train , ))) test_set = tf . data . Dataset . from_tensor_slices ((( x_test , ), ( y_test , ))) clf = ak . ImageClassifier ( max_trials = 10 ) # Feed the tensorflow Dataset to the classifier. clf . fit ( train_set ) # Predict with the best model. predicted_y = clf . predict ( test_set ) # Evaluate the best model with testing data. print ( clf . evaluate ( test_set )) Reference ImageClassifier , AutoModel , ImageBlock , Normalization , ImageAugmentation , ResNetBlock , ImageInput , ClassificationHead .","title":"Image Classification"},{"location":"tutorial/image_classification/#image-classification","text":"","title":"Image Classification"},{"location":"tutorial/image_classification/#a-simple-example","text":"The first step is to prepare your data. Here we use the MNIST dataset as an example. from tensorflow.keras.datasets import mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () print ( x_train . shape ) # (60000, 28, 28) print ( y_train . shape ) # (60000,) print ( y_train [: 3 ]) # array([7, 2, 1], dtype=uint8) The second step is to run the ImageClassifier . import autokeras as ak # Initialize the image classifier. clf = ak . ImageClassifier ( max_trials = 10 ) # It tries 10 different models. # Feed the image classifier with training data. clf . fit ( x_train , y_train ) # Predict with the best model. predicted_y = clf . predict ( x_test ) # Evaluate the best model with testing data. print ( clf . evaluate ( x_test , y_test ))","title":"A Simple Example"},{"location":"tutorial/image_classification/#validation-data","text":"By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. clf . fit ( x_train , y_train , # Split the training data and use the last 15% as validation data. validation_split = 0.15 ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 50000 x_val = x_train [ split :] y_val = y_train [ split :] x_train = x_train [: split ] y_train = y_train [: split ] clf . fit ( x_train , y_train , # Use your own validation set. validation_data = ( x_val , y_val ))","title":"Validation Data"},{"location":"tutorial/image_classification/#customized-search-space","text":"For advanced users, you may customize your search space by using AutoModel instead of ImageClassifier . You can configure the ImageBlock for some high-level configurations, e.g., block_type for the type of neural network to search, normalize for whether to do data normalization, augment for whether to do data augmentation. You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail. import autokeras as ak input_node = ak . ImageInput () output_node = ak . ImageBlock ( # Only search ResNet architectures. block_type = 'resnet' , # Normalize the dataset. normalize = True , # Do not do data augmentation. augment = False )( input_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) clf . fit ( x_train , y_train ) The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak.[some_block]([block_args])(input_node) . You can even also use more fine grained blocks to customize the search space even further. See the following example. import autokeras as ak input_node = ak . ImageInput () output_node = ak . Normalization ()( input_node ) output_node = ak . ImageAugmentation ( percentage = 0.3 )( output_node ) output_node = ak . ResNetBlock ( version = 'v2' )( output_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) clf . fit ( x_train , y_train )","title":"Customized Search Space"},{"location":"tutorial/image_classification/#data-format","text":"The AutoKeras ImageClassifier is quite flexible for the data format. For the image, it accepts data formats both with and without the channel dimension. The images in the MNIST dataset do not have the channel dimension. Each image is a matrix with shape (28, 28). AutoKeras also accepts images of three dimensions with the channel dimension at last, e.g., (32, 32, 3), (28, 28, 1). For the classification labels, AutoKeras accepts both plain labels, i.e. strings or integers, and one-hot encoded encoded labels, i.e. vectors of 0s and 1s. So if you prepare your data in the following way, the ImageClassifier should still work. from tensorflow.keras.datasets import mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () # Reshape the images to have the channel dimension. x_train = x_train . reshape ( x_train . shape + ( 1 ,)) x_test = x_test . reshape ( x_test . shape + ( 1 ,)) # One-hot encode the labels. import numpy as np eye = np . eye ( 10 ) y_train = eye [ y_train ] y_test = eye [ y_test ] print ( x_train . shape ) # (60000, 28, 28, 1) print ( y_train . shape ) # (60000, 10) print ( y_train [: 3 ]) # array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], # [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], # [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]]) We also support using tf.data.Dataset format for the training data. In this case, the images would have to be 3-dimentional. The labels have to be one-hot encoded for multi-class classification to be wrapped into tensorflow Dataset. import tensorflow as tf train_set = tf . data . Dataset . from_tensor_slices ((( x_train , ), ( y_train , ))) test_set = tf . data . Dataset . from_tensor_slices ((( x_test , ), ( y_test , ))) clf = ak . ImageClassifier ( max_trials = 10 ) # Feed the tensorflow Dataset to the classifier. clf . fit ( train_set ) # Predict with the best model. predicted_y = clf . predict ( test_set ) # Evaluate the best model with testing data. print ( clf . evaluate ( test_set ))","title":"Data Format"},{"location":"tutorial/image_classification/#reference","text":"ImageClassifier , AutoModel , ImageBlock , Normalization , ImageAugmentation , ResNetBlock , ImageInput , ClassificationHead .","title":"Reference"},{"location":"tutorial/image_regression/","text":"We are still working on this tutorial. Thank you for your patience!","title":"Image Regression"},{"location":"tutorial/multi/","text":"In this tutorial we are making use of the AutoModel API to show how to handle multi-modal data and multi-task. What is multi-modal? Multi-model data means each data instance has multiple forms of information. For example, a photo can be saved as a image. Besides the image, it may also have when and where it was taken as its attributes, which can be represented as structured data. What is multi-task? Multi-task here we refer to we want to predict multiple targets with the same input features. For example, we not only want to classify an image according to its content, but we also want to regress its quality as a float number between 0 and 1. The following diagram shows an example of multi-modal and multi-task neural network model. graph TD id1(ImageInput) --> id3(Some Neural Network Model) id2(StructuredDataInput) --> id3 id3 --> id4(ClassificationHead) id3 --> id5(RegressionHead) It has two inputs the images and the structured data. Each image is associated with a set of attributes in the structured data. From these data, we are trying to predict the classification label and the regression value at the same time. Data Preparation To illustrate our idea, we generate some random image and structured data as the multi-modal data. num_instances = 100 # Generate image data. image_data = np . random . rand ( num_instances , 32 , 32 , 3 ) . astype ( np . float32 ) # Generate structured data. structured_data = np . random . rand ( num_instances , 20 ) . astype ( np . float32 ) We also generate some multi-task targets for classification and regression. # Generate regression targets. regression_target = np . random . rand ( num_instances , 1 ) . astype ( np . float32 ) # Generate classification labels of five classes. classification_target = np . random . randint ( 5 , size = num_instances ) Build and Train the Model Then we initialize the multi-modal and multi-task model with AutoModel . import autokeras as ak # Initialize the multi with multiple inputs and outputs. model = ak . AutoModel ( inputs = [ ak . ImageInput (), ak . StructuredDataInput ()], outputs = [ ak . RegressionHead ( metrics = [ 'mae' ]), ak . ClassificationHead ( loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ]) ], max_trials = 10 ) # Fit the model with prepared data. model . fit ( [ image_data , structured_data ], [ regression_target , classification_target ], epochs = 10 ) Validation Data By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. model . fit ( [ image_data , structured_data ], [ regression_target , classification_target ], # Split the training data and use the last 15% as validation data. validation_split = 0.15 ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 20 image_val = image_data [ split :] structured_val = structured_data [ split :] regression_val = regression_target [ split :] classification_val = classification_target [ split :] image_data = image_data [: split ] structured_data = structured_data [: split ] regression_target = regression_target [: split ] classification_target = classification_target [: split ] model . fit ( x_train , y_train , # Use your own validation set. validation_data = ( x_val , y_val )) Customized Search Space You can customize your search space. The following figure shows the search space we want to define. graph LR id1(ImageInput) --> id2(Normalization) id2 --> id3(Image Augmentation) id3 --> id4(Convolutional) id3 --> id5(ResNet V2) id4 --> id6(Merge) id5 --> id6 id7(StructuredDataInput) --> id8(CategoricalToNumerical) id8 --> id9(DenseBlock) id6 --> id10(Merge) id9 --> id10 id10 --> id11(Classification Head) id10 --> id12(Regression Head) import autokeras as ak input_node1 = ak . ImageInput () output_node = ak . Normalization ()( input_node1 ) output_node = ak . ImageAugmentation ()( output_node ) output_node1 = ak . ConvBlock ()( output_node ) output_node2 = ak . ResNetBlock ( version = 'v2' )( output_node ) output_node1 = ak . Merge ()([ output_node1 , output_node2 ]) input_node2 = ak . StructuredDataInput () output_node = ak . CategoricalToNumerical ()( input_node2 ) output_node2 = ak . DenseBlock ()( output_node ) output_node = ak . Merge ()([ output_node1 , output_node2 ]) output_node1 = ak . ClassificationHead ()( output_node ) output_node2 = ak . RegressionHead ()( output_node ) auto_model = ak . AutoModel ( inputs = [ input_node1 , input_node2 ], outputs = [] output_node1 , output_node2 ], max_trials = 10 ) Data Format You can refer to the documentation of ImageInput , StructuredDataInput , TextInput , RegressionHead , ClassificationHead , for the format of different types of data. You can also refer to the Data Format section of the tutorials of Image Classification , Text Classification , Structured Data Classification . Reference AutoModel , ImageInput , StructuredDataInput , DenseBlock , RegressionHead , ClassificationHead , CategoricalToNumerical .","title":"Multi-Modal and Multi-Task"},{"location":"tutorial/multi/#what-is-multi-modal","text":"Multi-model data means each data instance has multiple forms of information. For example, a photo can be saved as a image. Besides the image, it may also have when and where it was taken as its attributes, which can be represented as structured data.","title":"What is multi-modal?"},{"location":"tutorial/multi/#what-is-multi-task","text":"Multi-task here we refer to we want to predict multiple targets with the same input features. For example, we not only want to classify an image according to its content, but we also want to regress its quality as a float number between 0 and 1. The following diagram shows an example of multi-modal and multi-task neural network model. graph TD id1(ImageInput) --> id3(Some Neural Network Model) id2(StructuredDataInput) --> id3 id3 --> id4(ClassificationHead) id3 --> id5(RegressionHead) It has two inputs the images and the structured data. Each image is associated with a set of attributes in the structured data. From these data, we are trying to predict the classification label and the regression value at the same time.","title":"What is multi-task?"},{"location":"tutorial/multi/#data-preparation","text":"To illustrate our idea, we generate some random image and structured data as the multi-modal data. num_instances = 100 # Generate image data. image_data = np . random . rand ( num_instances , 32 , 32 , 3 ) . astype ( np . float32 ) # Generate structured data. structured_data = np . random . rand ( num_instances , 20 ) . astype ( np . float32 ) We also generate some multi-task targets for classification and regression. # Generate regression targets. regression_target = np . random . rand ( num_instances , 1 ) . astype ( np . float32 ) # Generate classification labels of five classes. classification_target = np . random . randint ( 5 , size = num_instances )","title":"Data Preparation"},{"location":"tutorial/multi/#build-and-train-the-model","text":"Then we initialize the multi-modal and multi-task model with AutoModel . import autokeras as ak # Initialize the multi with multiple inputs and outputs. model = ak . AutoModel ( inputs = [ ak . ImageInput (), ak . StructuredDataInput ()], outputs = [ ak . RegressionHead ( metrics = [ 'mae' ]), ak . ClassificationHead ( loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ]) ], max_trials = 10 ) # Fit the model with prepared data. model . fit ( [ image_data , structured_data ], [ regression_target , classification_target ], epochs = 10 )","title":"Build and Train the Model"},{"location":"tutorial/multi/#validation-data","text":"By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. model . fit ( [ image_data , structured_data ], [ regression_target , classification_target ], # Split the training data and use the last 15% as validation data. validation_split = 0.15 ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 20 image_val = image_data [ split :] structured_val = structured_data [ split :] regression_val = regression_target [ split :] classification_val = classification_target [ split :] image_data = image_data [: split ] structured_data = structured_data [: split ] regression_target = regression_target [: split ] classification_target = classification_target [: split ] model . fit ( x_train , y_train , # Use your own validation set. validation_data = ( x_val , y_val ))","title":"Validation Data"},{"location":"tutorial/multi/#customized-search-space","text":"You can customize your search space. The following figure shows the search space we want to define. graph LR id1(ImageInput) --> id2(Normalization) id2 --> id3(Image Augmentation) id3 --> id4(Convolutional) id3 --> id5(ResNet V2) id4 --> id6(Merge) id5 --> id6 id7(StructuredDataInput) --> id8(CategoricalToNumerical) id8 --> id9(DenseBlock) id6 --> id10(Merge) id9 --> id10 id10 --> id11(Classification Head) id10 --> id12(Regression Head) import autokeras as ak input_node1 = ak . ImageInput () output_node = ak . Normalization ()( input_node1 ) output_node = ak . ImageAugmentation ()( output_node ) output_node1 = ak . ConvBlock ()( output_node ) output_node2 = ak . ResNetBlock ( version = 'v2' )( output_node ) output_node1 = ak . Merge ()([ output_node1 , output_node2 ]) input_node2 = ak . StructuredDataInput () output_node = ak . CategoricalToNumerical ()( input_node2 ) output_node2 = ak . DenseBlock ()( output_node ) output_node = ak . Merge ()([ output_node1 , output_node2 ]) output_node1 = ak . ClassificationHead ()( output_node ) output_node2 = ak . RegressionHead ()( output_node ) auto_model = ak . AutoModel ( inputs = [ input_node1 , input_node2 ], outputs = [] output_node1 , output_node2 ], max_trials = 10 )","title":"Customized Search Space"},{"location":"tutorial/multi/#data-format","text":"You can refer to the documentation of ImageInput , StructuredDataInput , TextInput , RegressionHead , ClassificationHead , for the format of different types of data. You can also refer to the Data Format section of the tutorials of Image Classification , Text Classification , Structured Data Classification .","title":"Data Format"},{"location":"tutorial/multi/#reference","text":"AutoModel , ImageInput , StructuredDataInput , DenseBlock , RegressionHead , ClassificationHead , CategoricalToNumerical .","title":"Reference"},{"location":"tutorial/overview/","text":"AutoKeras 1.0 Tutorial Supported Tasks AutoKeras supports several tasks with extremely simple interface. You can click the links below to see the detailed tutorial for each task. Suported Tasks : Image Classification , Image Regression , Text Classification , Text Regression , Structured Data Classification , Structured Data Regression . Coming Soon : Time Series Forcasting, Object Detection, Image Segmentation. Multi-Task and Multi-Modal Data If you are dealing with multi-task or multi-modal dataset, you can refer to this tutorial for details. Customized Model Follow this tutorial , to use AutoKeras building blocks to quickly construct your own model. With these blocks, you only need to specify the high-level architecture of your model. AutoKeras would search for the best detailed configuration for you. Moreover, you can override the base classes to create your own block. The following are the links to the documentation of the predefined input nodes and blocks in AutoKeras. Nodes : ImageInput , Input , StructuredDataInput , TextInput . Preprocessors : ImageAugmentation , Normalization , TextToIntSequence , TextToNgramVector , CategoricalToNumerical . Blocks : ConvBlock , DenseBlock , Embedding , Merge , ResNetBlock , RNNBlock , SpatialReduction , TemporalReduction , XceptionBlock , ImageBlock , StructuredDataBlock , TextBlock . Heads : ClassificationHead , RegressionHead . Export Model You can follow this tutorial to export the best model.","title":"Overview"},{"location":"tutorial/overview/#autokeras-10-tutorial","text":"","title":"AutoKeras 1.0 Tutorial"},{"location":"tutorial/overview/#supported-tasks","text":"AutoKeras supports several tasks with extremely simple interface. You can click the links below to see the detailed tutorial for each task. Suported Tasks : Image Classification , Image Regression , Text Classification , Text Regression , Structured Data Classification , Structured Data Regression . Coming Soon : Time Series Forcasting, Object Detection, Image Segmentation.","title":"Supported Tasks"},{"location":"tutorial/overview/#multi-task-and-multi-modal-data","text":"If you are dealing with multi-task or multi-modal dataset, you can refer to this tutorial for details.","title":"Multi-Task and Multi-Modal Data"},{"location":"tutorial/overview/#customized-model","text":"Follow this tutorial , to use AutoKeras building blocks to quickly construct your own model. With these blocks, you only need to specify the high-level architecture of your model. AutoKeras would search for the best detailed configuration for you. Moreover, you can override the base classes to create your own block. The following are the links to the documentation of the predefined input nodes and blocks in AutoKeras. Nodes : ImageInput , Input , StructuredDataInput , TextInput . Preprocessors : ImageAugmentation , Normalization , TextToIntSequence , TextToNgramVector , CategoricalToNumerical . Blocks : ConvBlock , DenseBlock , Embedding , Merge , ResNetBlock , RNNBlock , SpatialReduction , TemporalReduction , XceptionBlock , ImageBlock , StructuredDataBlock , TextBlock . Heads : ClassificationHead , RegressionHead .","title":"Customized Model"},{"location":"tutorial/overview/#export-model","text":"You can follow this tutorial to export the best model.","title":"Export Model"},{"location":"tutorial/structured_data_classification/","text":"Structured Data Classification A Simple Example The first step is to prepare your data. Here we use the Titanic dataset as an example. You can download the CSV files here . The second step is to run the StructuredDataClassifier . Replace all the /path/to with the path to the csv files. import autokeras as ak # Initialize the structured data classifier. clf = ak . StructuredDataClassifier ( max_trials = 10 ) # It tries 10 different models. # Feed the structured data classifier with training data. clf . fit ( # The path to the train.csv file. '/path/to/train.csv' , # The name of the label column. 'survived' ) # Predict with the best model. predicted_y = clf . predict ( '/path/to/eval.csv' ) # Evaluate the best model with testing data. print ( clf . evaluate ( '/path/to/eval.csv' , 'survived' )) Data Format The AutoKeras StructuredDataClassifier is quite flexible for the data format. The example above shows how to use the CSV files directly. Besides CSV files, it also supports numpy.ndarray, pandas.DataFrame or tf.data.Dataset . The data should be two-dimensional with numerical or categorical values. For the classification labels, AutoKeras accepts both plain labels, i.e. strings or integers, and one-hot encoded encoded labels, i.e. vectors of 0s and 1s. The labels can be numpy.ndarray, pandas.DataFrame, or pandas.Series. The following examples show how the data can be prepared with numpy.ndarray, pandas.DataFrame, and tensorflow.data.Dataset. import pandas as pd # x_train as pandas.DataFrame, y_train as pandas.Series x_train = pd . read_csv ( 'train.csv' ) print ( type ( x_train )) # pandas.DataFrame y_train = x_train . pop ( 'survived' ) print ( type ( y_train )) # pandas.Series # You can also use pandas.DataFrame for y_train. y_train = pd . DataFrame ( y_train ) print ( type ( y_train )) # pandas.DataFrame # You can also use numpy.ndarray for x_train and y_train. x_train = x_train . to_numpy () y_train = y_train . to_numpy () print ( type ( x_train )) # numpy.ndarray print ( type ( y_train )) # numpy.ndarray # Preparing testing data. x_test = pd . read_csv ( 'eval.csv' ) y_test = x_test . pop ( 'survived' ) # It tries 10 different models. clf = ak . StructuredDataClassifier ( max_trials = 10 ) # Feed the structured data classifier with training data. clf . fit ( x_train , y_train ) # Predict with the best model. predicted_y = clf . predict ( x_test ) # Evaluate the best model with testing data. print ( clf . evaluate ( x_test , y_test )) The following code shows how to convert numpy.ndarray to tf.data.Dataset. Notably, the labels have to be one-hot encoded for multi-class classification to be wrapped into tensorflow Dataset. Since the Titanic dataset is binary classification, it should not be one-hot encoded. import tensorflow as tf train_set = tf . data . Dataset . from_tensor_slices ((( x_train , ), ( y_train , ))) test_set = tf . data . Dataset . from_tensor_slices ((( x_test , ), ( y_test , ))) clf = ak . StructuredDataClassifier ( max_trials = 10 ) # Feed the tensorflow Dataset to the classifier. clf . fit ( train_set ) # Predict with the best model. predicted_y = clf . predict ( test_set ) # Evaluate the best model with testing data. print ( clf . evaluate ( test_set )) You can also specify the column names and types for the data as follows. The column_names is optional if the training data already have the column names, e.g. pandas.DataFrame, CSV file. Any column, whose type is not specified will be inferred from the training data. # Initialize the structured data classifier. clf = ak . StructuredDataClassifier ( column_names = [ 'sex' , 'age' , 'n_siblings_spouses' , 'parch' , 'fare' , 'class' , 'deck' , 'embark_town' , 'alone' ], column_types = { 'sex' : 'categorical' , 'fare' : 'numerical' }, max_trials = 10 , # It tries 10 different models. ) Validation Data By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. clf . fit ( x_train , y_train , # Split the training data and use the last 15% as validation data. validation_split = 0.15 ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 500 x_val = x_train [ split :] y_val = y_train [ split :] x_train = x_train [: split ] y_train = y_train [: split ] clf . fit ( x_train , y_train , # Use your own validation set. validation_data = ( x_val , y_val )) Customized Search Space For advanced users, you may customize your search space by using AutoModel instead of StructuredDataClassifier . You can configure the StructuredDataBlock for some high-level configurations, e.g., categorical_encoding for whether to use the CategoricalToNumerical . You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail. import autokeras as ak input_node = ak . StructuredDataInput () output_node = ak . StructuredDataBlock ( categorical_encoding = True , block_type = 'dense' )( input_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) clf . fit ( x_train , y_train ) The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak.[some_block]([block_args])(input_node) . You can even also use more fine grained blocks to customize the search space even further. See the following example. import autokeras as ak input_node = ak . StructuredDataInput () output_node = ak . CategoricalToNumerical ()( input_node ) output_node = ak . DenseBlock ()( output_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) clf . fit ( x_train , y_train ) Reference StructuredDataClassifier , AutoModel , StructuredDataClassifier , StructuredDataBlock , DenseBlock , StructuredDataInput , ClassificationHead , CategoricalToNumerical .","title":"Structured Data Classification"},{"location":"tutorial/structured_data_classification/#structured-data-classification","text":"","title":"Structured Data Classification"},{"location":"tutorial/structured_data_classification/#a-simple-example","text":"The first step is to prepare your data. Here we use the Titanic dataset as an example. You can download the CSV files here . The second step is to run the StructuredDataClassifier . Replace all the /path/to with the path to the csv files. import autokeras as ak # Initialize the structured data classifier. clf = ak . StructuredDataClassifier ( max_trials = 10 ) # It tries 10 different models. # Feed the structured data classifier with training data. clf . fit ( # The path to the train.csv file. '/path/to/train.csv' , # The name of the label column. 'survived' ) # Predict with the best model. predicted_y = clf . predict ( '/path/to/eval.csv' ) # Evaluate the best model with testing data. print ( clf . evaluate ( '/path/to/eval.csv' , 'survived' ))","title":"A Simple Example"},{"location":"tutorial/structured_data_classification/#data-format","text":"The AutoKeras StructuredDataClassifier is quite flexible for the data format. The example above shows how to use the CSV files directly. Besides CSV files, it also supports numpy.ndarray, pandas.DataFrame or tf.data.Dataset . The data should be two-dimensional with numerical or categorical values. For the classification labels, AutoKeras accepts both plain labels, i.e. strings or integers, and one-hot encoded encoded labels, i.e. vectors of 0s and 1s. The labels can be numpy.ndarray, pandas.DataFrame, or pandas.Series. The following examples show how the data can be prepared with numpy.ndarray, pandas.DataFrame, and tensorflow.data.Dataset. import pandas as pd # x_train as pandas.DataFrame, y_train as pandas.Series x_train = pd . read_csv ( 'train.csv' ) print ( type ( x_train )) # pandas.DataFrame y_train = x_train . pop ( 'survived' ) print ( type ( y_train )) # pandas.Series # You can also use pandas.DataFrame for y_train. y_train = pd . DataFrame ( y_train ) print ( type ( y_train )) # pandas.DataFrame # You can also use numpy.ndarray for x_train and y_train. x_train = x_train . to_numpy () y_train = y_train . to_numpy () print ( type ( x_train )) # numpy.ndarray print ( type ( y_train )) # numpy.ndarray # Preparing testing data. x_test = pd . read_csv ( 'eval.csv' ) y_test = x_test . pop ( 'survived' ) # It tries 10 different models. clf = ak . StructuredDataClassifier ( max_trials = 10 ) # Feed the structured data classifier with training data. clf . fit ( x_train , y_train ) # Predict with the best model. predicted_y = clf . predict ( x_test ) # Evaluate the best model with testing data. print ( clf . evaluate ( x_test , y_test )) The following code shows how to convert numpy.ndarray to tf.data.Dataset. Notably, the labels have to be one-hot encoded for multi-class classification to be wrapped into tensorflow Dataset. Since the Titanic dataset is binary classification, it should not be one-hot encoded. import tensorflow as tf train_set = tf . data . Dataset . from_tensor_slices ((( x_train , ), ( y_train , ))) test_set = tf . data . Dataset . from_tensor_slices ((( x_test , ), ( y_test , ))) clf = ak . StructuredDataClassifier ( max_trials = 10 ) # Feed the tensorflow Dataset to the classifier. clf . fit ( train_set ) # Predict with the best model. predicted_y = clf . predict ( test_set ) # Evaluate the best model with testing data. print ( clf . evaluate ( test_set )) You can also specify the column names and types for the data as follows. The column_names is optional if the training data already have the column names, e.g. pandas.DataFrame, CSV file. Any column, whose type is not specified will be inferred from the training data. # Initialize the structured data classifier. clf = ak . StructuredDataClassifier ( column_names = [ 'sex' , 'age' , 'n_siblings_spouses' , 'parch' , 'fare' , 'class' , 'deck' , 'embark_town' , 'alone' ], column_types = { 'sex' : 'categorical' , 'fare' : 'numerical' }, max_trials = 10 , # It tries 10 different models. )","title":"Data Format"},{"location":"tutorial/structured_data_classification/#validation-data","text":"By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. clf . fit ( x_train , y_train , # Split the training data and use the last 15% as validation data. validation_split = 0.15 ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 500 x_val = x_train [ split :] y_val = y_train [ split :] x_train = x_train [: split ] y_train = y_train [: split ] clf . fit ( x_train , y_train , # Use your own validation set. validation_data = ( x_val , y_val ))","title":"Validation Data"},{"location":"tutorial/structured_data_classification/#customized-search-space","text":"For advanced users, you may customize your search space by using AutoModel instead of StructuredDataClassifier . You can configure the StructuredDataBlock for some high-level configurations, e.g., categorical_encoding for whether to use the CategoricalToNumerical . You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail. import autokeras as ak input_node = ak . StructuredDataInput () output_node = ak . StructuredDataBlock ( categorical_encoding = True , block_type = 'dense' )( input_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) clf . fit ( x_train , y_train ) The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak.[some_block]([block_args])(input_node) . You can even also use more fine grained blocks to customize the search space even further. See the following example. import autokeras as ak input_node = ak . StructuredDataInput () output_node = ak . CategoricalToNumerical ()( input_node ) output_node = ak . DenseBlock ()( output_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) clf . fit ( x_train , y_train )","title":"Customized Search Space"},{"location":"tutorial/structured_data_classification/#reference","text":"StructuredDataClassifier , AutoModel , StructuredDataClassifier , StructuredDataBlock , DenseBlock , StructuredDataInput , ClassificationHead , CategoricalToNumerical .","title":"Reference"},{"location":"tutorial/structured_data_regression/","text":"We are still working on this tutorial. Thank you for your patience!","title":"Structured Data Regression"},{"location":"tutorial/text_classification/","text":"Text Classification A Simple Example The first step is to prepare your data. Here we use the IMDB dataset as an example. import numpy as np from tensorflow.keras.datasets import imdb # Load the integer sequence the IMDB dataset with Keras. index_offset = 3 # word index offset ( x_train , y_train ), ( x_test , y_test ) = imdb . load_data ( num_words = 1000 , index_from = index_offset ) y_train = y_train . reshape ( - 1 , 1 ) y_test = y_test . reshape ( - 1 , 1 ) # Prepare the dictionary of index to word. word_to_id = imdb . get_word_index () word_to_id = { k : ( v + index_offset ) for k , v in word_to_id . items ()} word_to_id [ \"<PAD>\" ] = 0 word_to_id [ \"<START>\" ] = 1 word_to_id [ \"<UNK>\" ] = 2 id_to_word = { value : key for key , value in word_to_id . items ()} # Convert the word indices to words. x_train = list ( map ( lambda sentence : ' ' . join ( id_to_word [ i ] for i in sentence ), x_train )) x_test = list ( map ( lambda sentence : ' ' . join ( id_to_word [ i ] for i in sentence ), x_test )) x_train = np . array ( x_train , dtype = np . str ) x_test = np . array ( x_test , dtype = np . str ) print ( x_train . shape ) # (25000,) print ( y_train . shape ) # (25000, 1) print ( x_train [ 0 ][: 50 ]) # <START> this film was just brilliant casting <UNK> The second step is to run the TextClassifier . import autokeras as ak # Initialize the text classifier. clf = ak . TextClassifier ( max_trials = 10 ) # It tries 10 different models. # Feed the text classifier with training data. clf . fit ( x_train , y_train ) # Predict with the best model. predicted_y = clf . predict ( x_test ) # Evaluate the best model with testing data. print ( clf . evaluate ( x_test , y_test )) Validation Data By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. clf . fit ( x_train , y_train , # Split the training data and use the last 15% as validation data. validation_split = 0.15 ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 5000 x_val = x_train [ split :] y_val = y_train [ split :] x_train = x_train [: split ] y_train = y_train [: split ] clf . fit ( x_train , y_train , # Use your own validation set. validation_data = ( x_val , y_val )) Customized Search Space For advanced users, you may customize your search space by using AutoModel instead of TextClassifier . You can configure the TextBlock for some high-level configurations, e.g., vectorizer for the type of text vectorization method to use. You can use 'sequence', which uses TextToInteSequence to convert the words to integers and use Embedding for embedding the integer sequences, or you can use 'ngram', which uses TextToNgramVector to vectorize the sentences. You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail. import autokeras as ak input_node = ak . TextInput () output_node = ak . TextBlock ( vectorizer = 'ngram' )( input_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) clf . fit ( x_train , y_train ) The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak.[some_block]([block_args])(input_node) . You can even also use more fine grained blocks to customize the search space even further. See the following example. import autokeras as ak input_node = ak . TextInput () output_node = ak . TextToIntSequence ()( input_node ) output_node = ak . Embedding ()( output_node ) # Use separable Conv layers in Keras. output_node = ak . ConvBlock ( separable = True )( output_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) clf . fit ( x_train , y_train ) Data Format The AutoKeras TextClassifier is quite flexible for the data format. For the text, the input data should be one-dimensional For the classification labels, AutoKeras accepts both plain labels, i.e. strings or integers, and one-hot encoded encoded labels, i.e. vectors of 0s and 1s. We also support using tf.data.Dataset format for the training data. The labels have to be one-hot encoded for multi-class classification to be wrapped into tensorflow Dataset. Since the IMDB dataset is binary classification, it should not be one-hot encoded. import tensorflow as tf train_set = tf . data . Dataset . from_tensor_slices ((( x_train , ), ( y_train , ))) test_set = tf . data . Dataset . from_tensor_slices ((( x_test , ), ( y_test , ))) clf = ak . TextClassifier ( max_trials = 10 ) # Feed the tensorflow Dataset to the classifier. clf . fit ( train_set ) # Predict with the best model. predicted_y = clf . predict ( test_set ) # Evaluate the best model with testing data. print ( clf . evaluate ( test_set )) Reference TextClassifier , AutoModel , TextBlock , TextToInteSequence , Embedding , TextToNgramVector , ConvBlock , TextInput , ClassificationHead .","title":"Text Classification"},{"location":"tutorial/text_classification/#text-classification","text":"","title":"Text Classification"},{"location":"tutorial/text_classification/#a-simple-example","text":"The first step is to prepare your data. Here we use the IMDB dataset as an example. import numpy as np from tensorflow.keras.datasets import imdb # Load the integer sequence the IMDB dataset with Keras. index_offset = 3 # word index offset ( x_train , y_train ), ( x_test , y_test ) = imdb . load_data ( num_words = 1000 , index_from = index_offset ) y_train = y_train . reshape ( - 1 , 1 ) y_test = y_test . reshape ( - 1 , 1 ) # Prepare the dictionary of index to word. word_to_id = imdb . get_word_index () word_to_id = { k : ( v + index_offset ) for k , v in word_to_id . items ()} word_to_id [ \"<PAD>\" ] = 0 word_to_id [ \"<START>\" ] = 1 word_to_id [ \"<UNK>\" ] = 2 id_to_word = { value : key for key , value in word_to_id . items ()} # Convert the word indices to words. x_train = list ( map ( lambda sentence : ' ' . join ( id_to_word [ i ] for i in sentence ), x_train )) x_test = list ( map ( lambda sentence : ' ' . join ( id_to_word [ i ] for i in sentence ), x_test )) x_train = np . array ( x_train , dtype = np . str ) x_test = np . array ( x_test , dtype = np . str ) print ( x_train . shape ) # (25000,) print ( y_train . shape ) # (25000, 1) print ( x_train [ 0 ][: 50 ]) # <START> this film was just brilliant casting <UNK> The second step is to run the TextClassifier . import autokeras as ak # Initialize the text classifier. clf = ak . TextClassifier ( max_trials = 10 ) # It tries 10 different models. # Feed the text classifier with training data. clf . fit ( x_train , y_train ) # Predict with the best model. predicted_y = clf . predict ( x_test ) # Evaluate the best model with testing data. print ( clf . evaluate ( x_test , y_test ))","title":"A Simple Example"},{"location":"tutorial/text_classification/#validation-data","text":"By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. clf . fit ( x_train , y_train , # Split the training data and use the last 15% as validation data. validation_split = 0.15 ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 5000 x_val = x_train [ split :] y_val = y_train [ split :] x_train = x_train [: split ] y_train = y_train [: split ] clf . fit ( x_train , y_train , # Use your own validation set. validation_data = ( x_val , y_val ))","title":"Validation Data"},{"location":"tutorial/text_classification/#customized-search-space","text":"For advanced users, you may customize your search space by using AutoModel instead of TextClassifier . You can configure the TextBlock for some high-level configurations, e.g., vectorizer for the type of text vectorization method to use. You can use 'sequence', which uses TextToInteSequence to convert the words to integers and use Embedding for embedding the integer sequences, or you can use 'ngram', which uses TextToNgramVector to vectorize the sentences. You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail. import autokeras as ak input_node = ak . TextInput () output_node = ak . TextBlock ( vectorizer = 'ngram' )( input_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) clf . fit ( x_train , y_train ) The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak.[some_block]([block_args])(input_node) . You can even also use more fine grained blocks to customize the search space even further. See the following example. import autokeras as ak input_node = ak . TextInput () output_node = ak . TextToIntSequence ()( input_node ) output_node = ak . Embedding ()( output_node ) # Use separable Conv layers in Keras. output_node = ak . ConvBlock ( separable = True )( output_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 10 ) clf . fit ( x_train , y_train )","title":"Customized Search Space"},{"location":"tutorial/text_classification/#data-format","text":"The AutoKeras TextClassifier is quite flexible for the data format. For the text, the input data should be one-dimensional For the classification labels, AutoKeras accepts both plain labels, i.e. strings or integers, and one-hot encoded encoded labels, i.e. vectors of 0s and 1s. We also support using tf.data.Dataset format for the training data. The labels have to be one-hot encoded for multi-class classification to be wrapped into tensorflow Dataset. Since the IMDB dataset is binary classification, it should not be one-hot encoded. import tensorflow as tf train_set = tf . data . Dataset . from_tensor_slices ((( x_train , ), ( y_train , ))) test_set = tf . data . Dataset . from_tensor_slices ((( x_test , ), ( y_test , ))) clf = ak . TextClassifier ( max_trials = 10 ) # Feed the tensorflow Dataset to the classifier. clf . fit ( train_set ) # Predict with the best model. predicted_y = clf . predict ( test_set ) # Evaluate the best model with testing data. print ( clf . evaluate ( test_set ))","title":"Data Format"},{"location":"tutorial/text_classification/#reference","text":"TextClassifier , AutoModel , TextBlock , TextToInteSequence , Embedding , TextToNgramVector , ConvBlock , TextInput , ClassificationHead .","title":"Reference"},{"location":"tutorial/text_regression/","text":"We are still working on this tutorial. Thank you for your patience!","title":"Text Regression"}]}